{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3064fc2fb04f4eb7a2e9b357d280faef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7adf014d4228428bb1ea9a729433436a",
              "IPY_MODEL_85448bab65c14a17945d28c804c66b61",
              "IPY_MODEL_b6031dc8c9244ccdbae94c63516c0c82"
            ],
            "layout": "IPY_MODEL_e05601e2f804486a9a64560e2574803c"
          }
        },
        "7adf014d4228428bb1ea9a729433436a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9c58104f94f4e3e9a98885b4ea33bc8",
            "placeholder": "​",
            "style": "IPY_MODEL_49a2296ceffe4804aaa71f7fb95134a1",
            "value": "config.json: 100%"
          }
        },
        "85448bab65c14a17945d28c804c66b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91d4ec9f1ad4db48ad9c3fb85505255",
            "max": 558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f48098dbe7f3445982981c94ec41f823",
            "value": 558
          }
        },
        "b6031dc8c9244ccdbae94c63516c0c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2927332bb1f4a299c6db372f78ba88e",
            "placeholder": "​",
            "style": "IPY_MODEL_eaa85f807ab944069c105e6a3dc9c0a4",
            "value": " 558/558 [00:00&lt;00:00, 38.9kB/s]"
          }
        },
        "e05601e2f804486a9a64560e2574803c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c58104f94f4e3e9a98885b4ea33bc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a2296ceffe4804aaa71f7fb95134a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b91d4ec9f1ad4db48ad9c3fb85505255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48098dbe7f3445982981c94ec41f823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2927332bb1f4a299c6db372f78ba88e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaa85f807ab944069c105e6a3dc9c0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10d9778c7da444f4b80ae87ee29c7335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca0e18e5134d40628dc5e53e8166d318",
              "IPY_MODEL_2ae732b6be05483cb3ddc7f919edcdad",
              "IPY_MODEL_45c89db10a754d85a8bfbdfce04cf41e"
            ],
            "layout": "IPY_MODEL_0533dfdbd4ca4251956c347b0b92d802"
          }
        },
        "ca0e18e5134d40628dc5e53e8166d318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ba03b32aba24cd898e9f9157ee85141",
            "placeholder": "​",
            "style": "IPY_MODEL_28b401ed08444166a8167571f9cf20d0",
            "value": "model.safetensors: 100%"
          }
        },
        "2ae732b6be05483cb3ddc7f919edcdad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f10ee86a0f644cba9ff210540b2c6280",
            "max": 9433012,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_118a35e1a247440ba7365e249102b822",
            "value": 9433012
          }
        },
        "45c89db10a754d85a8bfbdfce04cf41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39f950dfaca24145a4ad125820a2fb25",
            "placeholder": "​",
            "style": "IPY_MODEL_20115fd36241478287b0335bec300dc8",
            "value": " 9.43M/9.43M [00:00&lt;00:00, 57.0MB/s]"
          }
        },
        "0533dfdbd4ca4251956c347b0b92d802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba03b32aba24cd898e9f9157ee85141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28b401ed08444166a8167571f9cf20d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f10ee86a0f644cba9ff210540b2c6280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118a35e1a247440ba7365e249102b822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39f950dfaca24145a4ad125820a2fb25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20115fd36241478287b0335bec300dc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikidata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "cbz6lsIPctHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801f128c-5cd3-4dbc-ecae-2d79ba4ae0a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikidata\n",
            "  Downloading Wikidata-0.8.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Downloading Wikidata-0.8.1-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: Wikidata\n",
            "Successfully installed Wikidata-0.8.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2BXVShMsA0-4"
      },
      "outputs": [],
      "source": [
        "import torch, random, requests, os, pickle\n",
        "import numpy as np\n",
        "from wikidata.client import Client\n",
        "from datasets import load_dataset\n",
        "from itertools import islice\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "lEcH5MUeGVTR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(file_name, result):\n",
        "    # remove dump files if present\n",
        "    if os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "    with open(file_name, 'wb') as file:\n",
        "        print(\"dumping\", file_name)\n",
        "        # noinspection PyTypeChecker\n",
        "        pickle.dump(result, file)\n",
        "\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        print(\"loading\", file_name)\n",
        "        # noinspection PyTypeChecker\n",
        "        return pickle.load(file)"
      ],
      "metadata": {
        "id": "xCas4n7JePkX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia_pages(sitelinks):\n",
        "    result = []\n",
        "    for site_key in sitelinks.keys():\n",
        "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "            lang = site_key.replace(\"wiki\", \"\")\n",
        "            result.append(lang)\n",
        "    return result\n",
        "\n",
        "def build_claims(claims):\n",
        "    result = {}\n",
        "    for prop_id, values in claims.items():\n",
        "        result[prop_id] = len(values)\n",
        "    return result\n",
        "\n",
        "class Entity:\n",
        "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
        "        self.entity_id = entity_id\n",
        "        self.label = dataset_item['label']\n",
        "        self.name = dataset_item['name']\n",
        "        self.description = dataset_item['description']\n",
        "        self.type = dataset_item['type']\n",
        "        self.category = dataset_item['category']\n",
        "        self.subcategory = dataset_item['subcategory']\n",
        "        self.wiki_text = wiki_text\n",
        "        # Languages\n",
        "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
        "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
        "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
        "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
        "        # Properties\n",
        "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
        "\n",
        "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wiki_text(en_wiki):\n",
        "    if not en_wiki:\n",
        "        return None\n",
        "    title = en_wiki[\"title\"]\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
        "    res = requests.get(API_URL, params=params, headers=headers)\n",
        "    json = res.json()\n",
        "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
        "    # Keep the original text as it is.\n",
        "    # The text will be processed in other methods,\n",
        "    # such as processed_dataset#tokenize().\n",
        "    return page.get(\"extract\", \"\")\n",
        "\n",
        "class EntityFactory:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, item):\n",
        "        entity_id = extract_entity_id(item['item'])\n",
        "        try:\n",
        "            wikidata = self.client.get(entity_id, load=True)\n",
        "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
        "            en_wiki = sitelinks.get(\"enwiki\")\n",
        "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
        "        except Exception as e:\n",
        "            print(\"Error loading id:\", entity_id, e)\n",
        "            return None\n",
        "\n",
        "TRAINING_FILE_NAME = \"training.bin\"\n",
        "VALIDATION_FILE_NAME = \"validation.bin\"\n",
        "\n",
        "def create_set(dataset, factory, limit, file_name):\n",
        "    # apply the limits\n",
        "    if limit is None:\n",
        "        limit = len(dataset)\n",
        "    result = []\n",
        "    for index, item in enumerate(islice(dataset, limit)):\n",
        "        created = factory.create(item)\n",
        "        if created is not None:\n",
        "            result.append(created)\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
        "    return result\n",
        "\n",
        "class NLPDataset:\n",
        "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
        "        if not (os.path.exists(TRAINING_FILE_NAME)) or not (os.path.exists(VALIDATION_FILE_NAME)) or force_reload:\n",
        "            # load the project dataset\n",
        "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HUGGINGFACE_TOKEN'))\n",
        "            # a factory object is used to create our entities\n",
        "            factory = EntityFactory(Client())\n",
        "\n",
        "            self.training_set = create_set(dataset['train'], factory, training_limit, TRAINING_FILE_NAME)\n",
        "            self.validation_set = create_set(dataset['validation'], factory, validation_limit, VALIDATION_FILE_NAME)\n",
        "            dump(TRAINING_FILE_NAME, self.training_set)\n",
        "            dump(VALIDATION_FILE_NAME, self.validation_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.training_set = load(TRAINING_FILE_NAME)\n",
        "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set))"
      ],
      "metadata": {
        "id": "jaK4aqHbMhFs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_dataset = NLPDataset()"
      ],
      "metadata": {
        "id": "2oVOAmBrhwPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da978569-a8c3-4e3e-a74e-50edbd5e112c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training.bin\n",
            "loading validation.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "id": "MxmjT68Nxt2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68251562-6588-498d-efe4-bcaaeb38ba63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchtext==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchtext==0.6.0) (3.0.2)\n",
            "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections, nltk, string\n",
        "import pandas as pd\n",
        "from torchtext.vocab import GloVe\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from torch.utils.data import IterableDataset"
      ],
      "metadata": {
        "id": "eR3AbdkjxJRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def type_vector(base_type):\n",
        "    vector = np.zeros(1, dtype=np.float32)\n",
        "    if base_type == 'entity':\n",
        "        vector[0] = vector[0] + 1\n",
        "    return vector\n",
        "\n",
        "def label_to_number(label):\n",
        "    if label == 'cultural agnostic':\n",
        "        return 0\n",
        "    if label == 'cultural representative':\n",
        "        return 1\n",
        "    if label == 'cultural exclusive':\n",
        "        return 2\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "class ProcessedEntity:\n",
        "    def __init__(self, base: Entity, desc_text, wiki_text):\n",
        "        self.base_entity = base.entity_id + \": \" + base.name\n",
        "        # processed fields\n",
        "        self.desc_text = desc_text\n",
        "        self.wiki_text = wiki_text\n",
        "        self.labels_text = base.labels\n",
        "        self.descriptions_text = base.descriptions\n",
        "        self.aliases_text = base.aliases\n",
        "        self.pages_text = base.wikipedia_pages\n",
        "        # Using map to denote a Python dictionary,\n",
        "        # since the dictionary is already use for a word (term) dictionary\n",
        "        self.claims_map = base.claims\n",
        "        self.subcategory = base.subcategory\n",
        "        self.category = base.category\n",
        "\n",
        "        # build later (then the dictionaries are finalized)\n",
        "        self.desc_vector = None\n",
        "        self.wiki_vector = None\n",
        "        self.labels_vector = None\n",
        "        self.descriptions_vector = None\n",
        "        self.aliases_vector = None\n",
        "        self.pages_vector = None\n",
        "        self.claims_vector = None\n",
        "        # it includes implicitly the category\n",
        "        # since the subcategory values have been ordered by category\n",
        "        self.subcategory_vector = None\n",
        "        # in this case we can assume that we have only two types (entity vs concept)\n",
        "        self.type_vector = type_vector(base.type)\n",
        "        self.desc_glove_vector = None\n",
        "        self.output_label = label_to_number(base.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.base_entity + \" < \" + str(len(self.desc_text)) + \", \" + str(len(self.wiki_text)) + \" >\"\n",
        "\n",
        "    def dataset_item(self):\n",
        "        return {\n",
        "            \"desc\" : self.desc_vector,\n",
        "            \"wiki\" : self.wiki_vector,\n",
        "            \"labels\" : self.labels_vector,\n",
        "            \"descriptions\" : self.descriptions_vector,\n",
        "            \"aliases\" : self.aliases_vector,\n",
        "            \"pages\" : self.pages_vector,\n",
        "            \"claims\" : self.claims_vector,\n",
        "            \"category\" : self.subcategory_vector,\n",
        "            \"type\" : self.type_vector,\n",
        "            \"desc_glove\" : self.desc_glove_vector,\n",
        "            \"output_label\" : self.output_label,\n",
        "            \"base\" : self.base_entity\n",
        "        }\n",
        "\n",
        "class CategoryTable:\n",
        "    def __init__(self):\n",
        "        self.subcategories_entered = {}  # to avoid duplicates\n",
        "        self.subcategories = []\n",
        "        self.categories = []\n",
        "        self.subcategory_to_id = None  # computed on build\n",
        "\n",
        "    def include(self, processed_entity: ProcessedEntity):\n",
        "        if processed_entity.subcategory in self.subcategories_entered:\n",
        "            return\n",
        "        self.subcategories_entered[processed_entity.subcategory] = True\n",
        "        self.subcategories.append(processed_entity.subcategory)\n",
        "        self.categories.append(processed_entity.category)\n",
        "\n",
        "    def build(self):\n",
        "        data = {\n",
        "            'subcategory': self.subcategories,\n",
        "            'category': self.categories\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        df = df.sort_values('category')\n",
        "        print(df.to_markdown())\n",
        "        self.subcategory_to_id = {row[\"subcategory\"]: index for index, (_, row) in enumerate(df.iterrows())}\n",
        "\n",
        "    def subcat_to_vector(self, subcategory):\n",
        "        vector = np.zeros(1, dtype=np.float32)\n",
        "        vector[0] = vector[0] + self.subcategory_to_id[subcategory]\n",
        "        return vector\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.subcategory_to_id)\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self):\n",
        "        self.occurrences = []\n",
        "        self.unk_token = None\n",
        "        self.word_to_id = None\n",
        "\n",
        "    def include(self, tokenized_text):\n",
        "        self.occurrences.extend(tokenized_text)\n",
        "\n",
        "    def build(self, max_vocab_size, unk_token):\n",
        "        self.unk_token = unk_token\n",
        "        counter = collections.Counter(self.occurrences)\n",
        "        self.word_to_id = {key: index for index, (key, _) in enumerate(counter.most_common(max_vocab_size - 1))}\n",
        "        assert unk_token not in self.word_to_id\n",
        "        self.word_to_id[unk_token] = max_vocab_size - 1\n",
        "        self.occurrences = None # free memory space\n",
        "\n",
        "    def build_no_limits(self):\n",
        "        counter = collections.Counter(self.occurrences)\n",
        "        self.word_to_id = {key: index for index, (key, _) in enumerate(counter.most_common())}\n",
        "        self.occurrences = None # free memory space\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.word_to_id)\n",
        "\n",
        "    def words_to_vector(self, words):\n",
        "        vector = np.zeros(self.length(), dtype=np.float32)\n",
        "        for word in words:\n",
        "            if word == self.unk_token:\n",
        "                continue\n",
        "            if word in self.word_to_id:\n",
        "                vector[self.word_to_id[word]] = vector[self.word_to_id[word]] + 1\n",
        "        return vector\n",
        "\n",
        "    # Using map to denote a Python dictionary,\n",
        "    # since the dictionary is already use for a word (term) dictionary\n",
        "    def map_to_vector(self, dictionary):\n",
        "        vector = np.zeros(self.length(), dtype=np.float32)\n",
        "        for word, freq in dictionary.items():\n",
        "            if word == self.unk_token:\n",
        "                continue\n",
        "            if word in self.word_to_id:\n",
        "                vector[self.word_to_id[word]] = freq\n",
        "        return vector\n",
        "\n",
        "PAD_TOKEN = '<PAD>'\n",
        "\n",
        "class GloveProcessing:\n",
        "    def __init__(self, context_size):\n",
        "        self.glove = GloVe(name='6B', dim=100)\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def words_to_vect(self, words):\n",
        "        return self.glove.get_vecs_by_tokens(self.tokens(words)).view(-1)\n",
        "\n",
        "    def tokens(self, words):\n",
        "        return words[:self.context_size] + [PAD_TOKEN]*(self.context_size-len(words))\n",
        "\n",
        "UNK = '<UNK>' # the token to be used for out of vocabulary words\n",
        "DESC_VOCAB_SIZE = 4_000\n",
        "WIKI_VOCAB_SIZE = 10_000\n",
        "GLOVE_EMBEDDING_SIZE = 20\n",
        "\n",
        "class Dictionaries:\n",
        "    def __init__(self):\n",
        "        # descriptions and wiki text words are in 2 different vector spaces\n",
        "        self.desc = Dictionary()\n",
        "        self.wiki = Dictionary()\n",
        "        # we use the same languages keys dictionaries for:\n",
        "        # labels_text, descriptions_text, aliases_text, pages_text\n",
        "        self.languages = Dictionary()\n",
        "        self.claims = Dictionary()\n",
        "        self.category_table = CategoryTable()\n",
        "        # extra add glove embeddings\n",
        "        self.glove_desc = GloveProcessing(GLOVE_EMBEDDING_SIZE)\n",
        "\n",
        "    def include(self, processed_entity: ProcessedEntity):\n",
        "        self.desc.include(processed_entity.desc_text)\n",
        "        self.wiki.include(processed_entity.wiki_text)\n",
        "        self.languages.include(processed_entity.labels_text)\n",
        "        self.languages.include(processed_entity.descriptions_text)\n",
        "        self.languages.include(processed_entity.aliases_text)\n",
        "        self.languages.include(processed_entity.pages_text)\n",
        "        self.claims.include(list(processed_entity.claims_map.keys()))\n",
        "        self.category_table.include(processed_entity)\n",
        "\n",
        "    def build(self):\n",
        "        self.desc.build(DESC_VOCAB_SIZE, UNK)\n",
        "        self.wiki.build(WIKI_VOCAB_SIZE, UNK)\n",
        "        self.claims.build_no_limits()\n",
        "        # those guys are not too large: so we can not limit them\n",
        "        self.languages.build_no_limits()\n",
        "        self.category_table.build()\n",
        "\n",
        "    def finalize(self, processed_entity: ProcessedEntity):\n",
        "        processed_entity.desc_vector = self.desc.words_to_vector(processed_entity.desc_text)\n",
        "        processed_entity.wiki_vector = self.wiki.words_to_vector(processed_entity.wiki_text)\n",
        "        processed_entity.labels_vector = self.languages.words_to_vector(processed_entity.labels_text)\n",
        "        processed_entity.descriptions_vector = self.languages.words_to_vector(processed_entity.descriptions_text)\n",
        "        processed_entity.aliases_vector = self.languages.words_to_vector(processed_entity.aliases_text)\n",
        "        processed_entity.pages_vector = self.languages.words_to_vector(processed_entity.pages_text)\n",
        "        processed_entity.claims_vector = self.claims.map_to_vector(processed_entity.claims_map)\n",
        "        processed_entity.subcategory_vector = self.category_table.subcat_to_vector(processed_entity.subcategory)\n",
        "        processed_entity.desc_glove_vector = self.glove_desc.words_to_vect(processed_entity.desc_text)\n",
        "\n",
        "class IterableEntities(IterableDataset):\n",
        "    def __init__(self, processed_entities: list[ProcessedEntity]):\n",
        "        self.processed_entities = processed_entities\n",
        "\n",
        "    def __iter__(self):\n",
        "        for entity in self.processed_entities:\n",
        "            yield entity.dataset_item()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_entities)\n",
        "\n",
        "TRAINING_PROC_FILE_NAME = \"training-proc.bin\"\n",
        "VALIDATION_PROC_FILE_NAME = \"validation-proc.bin\"\n",
        "\n",
        "def text_process(text, stop):\n",
        "    result = []\n",
        "    if text is None:\n",
        "        return result\n",
        "    for sentence in nltk.sent_tokenize(text.lower()):\n",
        "        result.extend([WordNetLemmatizer().lemmatize(i) for i in nltk.word_tokenize(sentence) if i not in stop])\n",
        "    return result\n",
        "\n",
        "def create_processed(entity, dictionaries, stop):\n",
        "    description_tokenized = text_process(entity.description, stop)\n",
        "    wiki_text_tokenized = text_process(entity.wiki_text, stop)\n",
        "    result = ProcessedEntity(entity, description_tokenized, wiki_text_tokenized)\n",
        "    dictionaries.include(result)\n",
        "    return result\n",
        "\n",
        "class ProcessedDataset(NLPDataset):\n",
        "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
        "        super().__init__(training_limit, validation_limit, force_reload)\n",
        "        if not (os.path.exists(TRAINING_PROC_FILE_NAME)) or not (os.path.exists(VALIDATION_PROC_FILE_NAME)) or force_reload:\n",
        "            self.processed_training_set, self.processed_validation_set = self.processing()\n",
        "            dump(TRAINING_PROC_FILE_NAME, self.processed_training_set)\n",
        "            dump(VALIDATION_PROC_FILE_NAME, self.processed_validation_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.processed_training_set = load(TRAINING_PROC_FILE_NAME)\n",
        "            self.processed_validation_set = load(VALIDATION_PROC_FILE_NAME)\n",
        "\n",
        "    def processing(self):\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('punkt_tab')\n",
        "        stop = set(stopwords.words('english') + list(string.punctuation) + ['==', \"''\", '``', \"'s\", '==='])\n",
        "        print(\"processing the data\")\n",
        "        dictionaries = Dictionaries()\n",
        "        # from the base data, add a list of processed entities\n",
        "        print(\"training set text processing started\")\n",
        "        processed_training_set = []\n",
        "        for index, entity in enumerate(self.training_set):\n",
        "            processed_training_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"training set processed\", index+1, \"entities\")\n",
        "        print(\"training set text processing ended\")\n",
        "        print(\"validation set text processing started\")\n",
        "        processed_validation_set = []\n",
        "        for index, entity in enumerate(self.validation_set):\n",
        "            processed_validation_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"validation set processed\", index+1, \"entities\")\n",
        "        print(\"validation set text processing ended\")\n",
        "        print(\"building dictionaries\")\n",
        "        # when we've collected all the words for the two spaces, we can build them\n",
        "        dictionaries.build()\n",
        "        print(\"text to vector started\")\n",
        "        # build the vectors from the texts\n",
        "        for entity in processed_training_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        for entity in processed_validation_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        print(\"text to vector finished\")\n",
        "        return processed_training_set, processed_validation_set\n",
        "\n",
        "    def training(self):\n",
        "        return IterableEntities(self.processed_training_set)\n",
        "\n",
        "    def validation(self):\n",
        "        return IterableEntities(self.processed_validation_set)"
      ],
      "metadata": {
        "id": "cLFV7M3iwrKf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = ProcessedDataset()"
      ],
      "metadata": {
        "id": "Er0coBDiQbom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda384f0-8bad-4738-fdd9-705e596af352"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training.bin\n",
            "loading validation.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing the data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.41MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:25<00:00, 15747.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set text processing started\n",
            "training set processed 100 entities\n",
            "training set processed 200 entities\n",
            "training set processed 300 entities\n",
            "training set processed 400 entities\n",
            "training set processed 500 entities\n",
            "training set processed 600 entities\n",
            "training set processed 700 entities\n",
            "training set processed 800 entities\n",
            "training set processed 900 entities\n",
            "training set processed 1000 entities\n",
            "training set processed 1100 entities\n",
            "training set processed 1200 entities\n",
            "training set processed 1300 entities\n",
            "training set processed 1400 entities\n",
            "training set processed 1500 entities\n",
            "training set processed 1600 entities\n",
            "training set processed 1700 entities\n",
            "training set processed 1800 entities\n",
            "training set processed 1900 entities\n",
            "training set processed 2000 entities\n",
            "training set processed 2100 entities\n",
            "training set processed 2200 entities\n",
            "training set processed 2300 entities\n",
            "training set processed 2400 entities\n",
            "training set processed 2500 entities\n",
            "training set processed 2600 entities\n",
            "training set processed 2700 entities\n",
            "training set processed 2800 entities\n",
            "training set processed 2900 entities\n",
            "training set processed 3000 entities\n",
            "training set processed 3100 entities\n",
            "training set processed 3200 entities\n",
            "training set processed 3300 entities\n",
            "training set processed 3400 entities\n",
            "training set processed 3500 entities\n",
            "training set processed 3600 entities\n",
            "training set processed 3700 entities\n",
            "training set processed 3800 entities\n",
            "training set processed 3900 entities\n",
            "training set processed 4000 entities\n",
            "training set processed 4100 entities\n",
            "training set processed 4200 entities\n",
            "training set processed 4300 entities\n",
            "training set processed 4400 entities\n",
            "training set processed 4500 entities\n",
            "training set processed 4600 entities\n",
            "training set processed 4700 entities\n",
            "training set processed 4800 entities\n",
            "training set processed 4900 entities\n",
            "training set processed 5000 entities\n",
            "training set processed 5100 entities\n",
            "training set processed 5200 entities\n",
            "training set processed 5300 entities\n",
            "training set processed 5400 entities\n",
            "training set processed 5500 entities\n",
            "training set processed 5600 entities\n",
            "training set processed 5700 entities\n",
            "training set processed 5800 entities\n",
            "training set processed 5900 entities\n",
            "training set processed 6000 entities\n",
            "training set processed 6100 entities\n",
            "training set processed 6200 entities\n",
            "training set text processing ended\n",
            "validation set text processing started\n",
            "validation set processed 100 entities\n",
            "validation set processed 200 entities\n",
            "validation set processed 300 entities\n",
            "validation set text processing ended\n",
            "building dictionaries\n",
            "|     | subcategory              | category                |\n",
            "|----:|:-------------------------|:------------------------|\n",
            "|  71 | architectural style      | architecture            |\n",
            "|   3 | building                 | architecture            |\n",
            "|  15 | architectural structure  | architecture            |\n",
            "|  52 | architect                | architecture            |\n",
            "|  51 | building material        | architecture            |\n",
            "|  27 | construction             | architecture            |\n",
            "|  29 | tree                     | biology                 |\n",
            "|  76 | biologist                | biology                 |\n",
            "|  84 | plant                    | biology                 |\n",
            "|  42 | organism                 | biology                 |\n",
            "|  40 | animal                   | biology                 |\n",
            "|  93 | fish                     | biology                 |\n",
            "|  97 | bookstore                | books                   |\n",
            "|  50 | non-fiction writer       | books                   |\n",
            "| 109 | religious book           | books                   |\n",
            "|   6 | book                     | books                   |\n",
            "|  33 | animated film            | comics and anime        |\n",
            "|  78 | comics artist            | comics and anime        |\n",
            "|  19 | animation studio         | comics and anime        |\n",
            "|  95 | manga                    | comics and anime        |\n",
            "|  12 | animation technique      | comics and anime        |\n",
            "|   2 | comics                   | comics and anime        |\n",
            "| 105 | fashion trend            | fashion                 |\n",
            "|  67 | clothing                 | fashion                 |\n",
            "|  79 | model                    | fashion                 |\n",
            "|  60 | traditional costume      | fashion                 |\n",
            "|  48 | designer                 | fashion                 |\n",
            "|  58 | textile                  | fashion                 |\n",
            "| 108 | acting style             | films                   |\n",
            "|  81 | film director            | films                   |\n",
            "|  13 | film festival            | films                   |\n",
            "|   0 | film                     | films                   |\n",
            "| 100 | film genre               | films                   |\n",
            "|  24 | film producer            | films                   |\n",
            "|  14 | drink                    | food                    |\n",
            "|  72 | dish                     | food                    |\n",
            "| 101 | cooking technique        | food                    |\n",
            "|  92 | ingredient               | food                    |\n",
            "| 103 | cook                     | food                    |\n",
            "|  99 | food                     | food                    |\n",
            "|  83 | mountain                 | geography               |\n",
            "|  49 | geographic location      | geography               |\n",
            "|   4 | city                     | geography               |\n",
            "| 104 | environment              | geography               |\n",
            "|  45 | neighborhood             | geography               |\n",
            "|  30 | river                    | geography               |\n",
            "|  63 | tradition                | gestures and habits     |\n",
            "| 107 | body language            | gestures and habits     |\n",
            "|  69 | gesture                  | gestures and habits     |\n",
            "| 110 | mores                    | gestures and habits     |\n",
            "|  37 | greeting                 | gestures and habits     |\n",
            "|  32 | ritual                   | gestures and habits     |\n",
            "|  77 | monument                 | history                 |\n",
            "|   5 | historical event         | history                 |\n",
            "|  25 | museum                   | history                 |\n",
            "|  75 | historian                | history                 |\n",
            "|  64 | archive                  | history                 |\n",
            "|  23 | poet                     | literature              |\n",
            "|  74 | literary genre           | literature              |\n",
            "|  89 | poetry                   | literature              |\n",
            "|  47 | publisher                | literature              |\n",
            "|  86 | literary award           | literature              |\n",
            "|  28 | writer                   | literature              |\n",
            "| 106 | writing style            | literature              |\n",
            "|   7 | film studio              | media                   |\n",
            "|  91 | streaming service        | media                   |\n",
            "|  90 | production company       | media                   |\n",
            "|  11 | television               | media                   |\n",
            "|   9 | media company            | media                   |\n",
            "|  10 | magazine                 | media                   |\n",
            "|  88 | record label             | music                   |\n",
            "|  55 | musician                 | music                   |\n",
            "|  39 | musical profession       | music                   |\n",
            "|  41 | music genre              | music                   |\n",
            "|  16 | music festival           | music                   |\n",
            "|   1 | musical group            | music                   |\n",
            "|  87 | theatrical genre         | performing arts         |\n",
            "|  46 | actor                    | performing arts         |\n",
            "|  80 | choreographer            | performing arts         |\n",
            "|  54 | theatrical director      | performing arts         |\n",
            "|  61 | dance                    | performing arts         |\n",
            "|  65 | folk dance               | performing arts         |\n",
            "|   8 | philosophical movement   | philosophy and religion |\n",
            "|  62 | religion                 | philosophy and religion |\n",
            "|  66 | religious movement       | philosophy and religion |\n",
            "|  22 | religious leader         | philosophy and religion |\n",
            "|  73 | philosopher              | philosophy and religion |\n",
            "|  59 | philosophy               | philosophy and religion |\n",
            "|  98 | government               | politics                |\n",
            "|  20 | politician               | politics                |\n",
            "|  43 | political party          | politics                |\n",
            "|  31 | government agency        | politics                |\n",
            "|  38 | law                      | politics                |\n",
            "|  36 | policy                   | politics                |\n",
            "|  21 | sports team              | sports                  |\n",
            "|  34 | recurring sporting event | sports                  |\n",
            "|  68 | sports equipment         | sports                  |\n",
            "|  35 | sports club              | sports                  |\n",
            "|  70 | sport                    | sports                  |\n",
            "|  53 | athlete                  | sports                  |\n",
            "|  26 | automobile manufacturer  | transportation          |\n",
            "|  44 | mode of transport        | transportation          |\n",
            "| 102 | transport                | transportation          |\n",
            "|  56 | station                  | transportation          |\n",
            "|  57 | transport company        | transportation          |\n",
            "|  94 | art movement             | visual arts             |\n",
            "|  85 | artist                   | visual arts             |\n",
            "|  18 | painting                 | visual arts             |\n",
            "|  17 | art gallery              | visual arts             |\n",
            "|  82 | photographer             | visual arts             |\n",
            "|  96 | visual arts              | visual arts             |\n",
            "| 111 | happening                | visual arts             |\n",
            "text to vector started\n",
            "text to vector finished\n",
            "dumping training-proc.bin\n",
            "dumping validation-proc.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, csv\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from huggingface_hub import PyTorchModelHubMixin"
      ],
      "metadata": {
        "id": "dKeySnraTKD3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_vector_layer(params):\n",
        "    in_features, out_features = params\n",
        "    # frequency vector fields rescaling (applying also a RuLU individually):\n",
        "    return nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
        "\n",
        "class MultiModalModel(nn.Module, PyTorchModelHubMixin,\n",
        "                      repo_url=\"fax4ever/culturalitems-no-transformer\",\n",
        "                      pipeline_tag=\"text-classification\",\n",
        "                      license=\"apache-2.0\"):\n",
        "    def __init__(self, params, device) -> None:\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.device = device\n",
        "        # individual input layers for frequency vectors\n",
        "        self.desc = rescale_vector_layer(params['desc']).to(device)\n",
        "        self.wiki = rescale_vector_layer(params['wiki']).to(device)\n",
        "        self.labels = rescale_vector_layer(params['labels']).to(device)\n",
        "        self.descriptions = rescale_vector_layer(params['descriptions']).to(device)\n",
        "        self.aliases = rescale_vector_layer(params['aliases']).to(device)\n",
        "        self.pages = rescale_vector_layer(params['pages']).to(device)\n",
        "        self.claims = rescale_vector_layer(params['claims']).to(device)\n",
        "        # individual input layers for scalar value\n",
        "        self.category = nn.Linear(params['category_dim'], params['category_scale']).to(device)\n",
        "        self.type_proj = nn.Linear(params['type_dim'], params['type_scale']).to(device)\n",
        "        # glove\n",
        "        self.desc_glove = rescale_vector_layer(params['desc_glove']).to(device)\n",
        "        # common classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(params['total_scale'], params['hidden_layers']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(params['dropout']),\n",
        "            nn.Linear(params['hidden_layers'], 3)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, dataset_items):\n",
        "        desc_feat = self.desc(dataset_items['desc'].to(self.device))\n",
        "        wiki_feat = self.wiki(dataset_items['wiki'].to(self.device))\n",
        "        labels_feat = self.labels(dataset_items['labels'].to(self.device))\n",
        "        descriptions_feat = self.descriptions(dataset_items['descriptions'].to(self.device))\n",
        "        aliases_feat = self.aliases(dataset_items['aliases'].to(self.device))\n",
        "        pages_feat = self.pages(dataset_items['pages'].to(self.device))\n",
        "        claims_feat = self.claims(dataset_items['claims'].to(self.device))\n",
        "        category_feat = self.category(dataset_items['category'].to(self.device))\n",
        "        type_feat = self.type_proj(dataset_items['type'].to(self.device))\n",
        "        desc_glove_feat = self.desc_glove(dataset_items['desc_glove'].to(self.device))\n",
        "        combined = torch.cat([desc_feat, desc_glove_feat, wiki_feat, labels_feat, descriptions_feat, aliases_feat, pages_feat,\n",
        "                              claims_feat, category_feat, type_feat], dim=1)\n",
        "        return self.classifier(combined)"
      ],
      "metadata": {
        "id": "ZRIukpPuTOAF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_label(label):\n",
        "    if label == 0:\n",
        "        return 'cultural agnostic'\n",
        "    if label == 1:\n",
        "        return 'cultural representative'\n",
        "    if label == 2:\n",
        "        return 'cultural exclusive'\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "model = MultiModalModel.from_pretrained(\"fax4ever/culturalitems-no-transformer\")\n",
        "matching = 0\n",
        "\n",
        "with open('no-transformer-inference.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    field = [\"entity\", \"true label\", \"prediction\", \"correct\"]\n",
        "    writer.writerow(field)\n",
        "    with torch.no_grad():\n",
        "        validation = ProcessedDataset().validation()\n",
        "        for entity in DataLoader(validation):\n",
        "            prediction = model(entity).detach().clone().argmax(dim=1).numpy()[0]\n",
        "            true_label = entity['output_label'].numpy()[0]\n",
        "            match = prediction == true_label\n",
        "            if match:\n",
        "                matching = matching + 1\n",
        "            base_ = entity['base'][0]\n",
        "            writer.writerow([base_, number_to_label(true_label), number_to_label(prediction), match])\n",
        "\n",
        "print('matched', matching, 'on', len(validation), '(', matching/len(validation), ')')"
      ],
      "metadata": {
        "id": "c8AsFndzTknl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "3064fc2fb04f4eb7a2e9b357d280faef",
            "7adf014d4228428bb1ea9a729433436a",
            "85448bab65c14a17945d28c804c66b61",
            "b6031dc8c9244ccdbae94c63516c0c82",
            "e05601e2f804486a9a64560e2574803c",
            "b9c58104f94f4e3e9a98885b4ea33bc8",
            "49a2296ceffe4804aaa71f7fb95134a1",
            "b91d4ec9f1ad4db48ad9c3fb85505255",
            "f48098dbe7f3445982981c94ec41f823",
            "c2927332bb1f4a299c6db372f78ba88e",
            "eaa85f807ab944069c105e6a3dc9c0a4",
            "10d9778c7da444f4b80ae87ee29c7335",
            "ca0e18e5134d40628dc5e53e8166d318",
            "2ae732b6be05483cb3ddc7f919edcdad",
            "45c89db10a754d85a8bfbdfce04cf41e",
            "0533dfdbd4ca4251956c347b0b92d802",
            "0ba03b32aba24cd898e9f9157ee85141",
            "28b401ed08444166a8167571f9cf20d0",
            "f10ee86a0f644cba9ff210540b2c6280",
            "118a35e1a247440ba7365e249102b822",
            "39f950dfaca24145a4ad125820a2fb25",
            "20115fd36241478287b0335bec300dc8"
          ]
        },
        "outputId": "e9341930-f3dc-4757-9fa3-fa364dbd555d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3064fc2fb04f4eb7a2e9b357d280faef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/9.43M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10d9778c7da444f4b80ae87ee29c7335"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training.bin\n",
            "loading validation.bin\n",
            "loading training-proc.bin\n",
            "loading validation-proc.bin\n",
            "matched 222 on 300 ( 0.74 )\n"
          ]
        }
      ]
    }
  ]
}