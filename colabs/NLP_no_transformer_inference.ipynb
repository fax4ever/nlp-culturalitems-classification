{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikidata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "cbz6lsIPctHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2BXVShMsA0-4"
      },
      "outputs": [],
      "source": [
        "import torch, random, requests, os, pickle\n",
        "import numpy as np\n",
        "from wikidata.client import Client\n",
        "from datasets import load_dataset\n",
        "from itertools import islice\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "lEcH5MUeGVTR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(file_name, result):\n",
        "    # remove dump files if present\n",
        "    if os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "    with open(file_name, 'wb') as file:\n",
        "        print(\"dumping\", file_name)\n",
        "        # noinspection PyTypeChecker\n",
        "        pickle.dump(result, file)\n",
        "\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        print(\"loading\", file_name)\n",
        "        # noinspection PyTypeChecker\n",
        "        return pickle.load(file)"
      ],
      "metadata": {
        "id": "xCas4n7JePkX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia_pages(sitelinks):\n",
        "    result = []\n",
        "    for site_key in sitelinks.keys():\n",
        "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "            lang = site_key.replace(\"wiki\", \"\")\n",
        "            result.append(lang)\n",
        "    return result\n",
        "\n",
        "def build_claims(claims):\n",
        "    result = {}\n",
        "    for prop_id, values in claims.items():\n",
        "        result[prop_id] = len(values)\n",
        "    return result\n",
        "\n",
        "class Entity:\n",
        "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
        "        self.entity_id = entity_id\n",
        "        self.label = dataset_item['label']\n",
        "        self.name = dataset_item['name']\n",
        "        self.description = dataset_item['description']\n",
        "        self.type = dataset_item['type']\n",
        "        self.category = dataset_item['category']\n",
        "        self.subcategory = dataset_item['subcategory']\n",
        "        self.wiki_text = wiki_text\n",
        "        # Languages\n",
        "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
        "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
        "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
        "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
        "        # Properties\n",
        "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
        "\n",
        "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wiki_text(en_wiki):\n",
        "    if not en_wiki:\n",
        "        return None\n",
        "    title = en_wiki[\"title\"]\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
        "    res = requests.get(API_URL, params=params, headers=headers)\n",
        "    json = res.json()\n",
        "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
        "    # Keep the original text as it is.\n",
        "    # The text will be processed in other methods,\n",
        "    # such as processed_dataset#tokenize().\n",
        "    return page.get(\"extract\", \"\")\n",
        "\n",
        "class EntityFactory:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, item):\n",
        "        entity_id = extract_entity_id(item['item'])\n",
        "        try:\n",
        "            wikidata = self.client.get(entity_id, load=True)\n",
        "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
        "            en_wiki = sitelinks.get(\"enwiki\")\n",
        "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
        "        except Exception as e:\n",
        "            print(\"Error loading id:\", entity_id, e)\n",
        "            return None\n",
        "\n",
        "TRAINING_FILE_NAME = \"training.bin\"\n",
        "VALIDATION_FILE_NAME = \"validation.bin\"\n",
        "\n",
        "def create_set(dataset, factory, limit, file_name):\n",
        "    # apply the limits\n",
        "    if limit is None:\n",
        "        limit = len(dataset)\n",
        "    result = []\n",
        "    for index, item in enumerate(islice(dataset, limit)):\n",
        "        created = factory.create(item)\n",
        "        if created is not None:\n",
        "            result.append(created)\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
        "    return result\n",
        "\n",
        "class NLPDataset:\n",
        "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
        "        if not (os.path.exists(TRAINING_FILE_NAME)) or not (os.path.exists(VALIDATION_FILE_NAME)) or force_reload:\n",
        "            # load the project dataset\n",
        "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HUGGINGFACE_TOKEN'))\n",
        "            # a factory object is used to create our entities\n",
        "            factory = EntityFactory(Client())\n",
        "\n",
        "            self.training_set = create_set(dataset['train'], factory, training_limit, TRAINING_FILE_NAME)\n",
        "            self.validation_set = create_set(dataset['validation'], factory, validation_limit, VALIDATION_FILE_NAME)\n",
        "            dump(TRAINING_FILE_NAME, self.training_set)\n",
        "            dump(VALIDATION_FILE_NAME, self.validation_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.training_set = load(TRAINING_FILE_NAME)\n",
        "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set))"
      ],
      "metadata": {
        "id": "jaK4aqHbMhFs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_dataset = NLPDataset()"
      ],
      "metadata": {
        "id": "2oVOAmBrhwPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "id": "MxmjT68Nxt2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections, nltk, string\n",
        "import pandas as pd\n",
        "from torchtext.vocab import GloVe\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from torch.utils.data import IterableDataset"
      ],
      "metadata": {
        "id": "eR3AbdkjxJRW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def type_vector(base_type):\n",
        "    vector = np.zeros(1, dtype=np.float32)\n",
        "    if base_type == 'entity':\n",
        "        vector[0] = vector[0] + 1\n",
        "    return vector\n",
        "\n",
        "def label_to_number(label):\n",
        "    if label == 'cultural agnostic':\n",
        "        return 0\n",
        "    if label == 'cultural representative':\n",
        "        return 1\n",
        "    if label == 'cultural exclusive':\n",
        "        return 2\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "class ProcessedEntity:\n",
        "    def __init__(self, base: Entity, desc_text, wiki_text):\n",
        "        self.base_entity = base.entity_id + \": \" + base.name\n",
        "        # processed fields\n",
        "        self.desc_text = desc_text\n",
        "        self.wiki_text = wiki_text\n",
        "        self.labels_text = base.labels\n",
        "        self.descriptions_text = base.descriptions\n",
        "        self.aliases_text = base.aliases\n",
        "        self.pages_text = base.wikipedia_pages\n",
        "        # Using map to denote a Python dictionary,\n",
        "        # since the dictionary is already use for a word (term) dictionary\n",
        "        self.claims_map = base.claims\n",
        "        self.subcategory = base.subcategory\n",
        "        self.category = base.category\n",
        "\n",
        "        # build later (then the dictionaries are finalized)\n",
        "        self.desc_vector = None\n",
        "        self.wiki_vector = None\n",
        "        self.labels_vector = None\n",
        "        self.descriptions_vector = None\n",
        "        self.aliases_vector = None\n",
        "        self.pages_vector = None\n",
        "        self.claims_vector = None\n",
        "        # it includes implicitly the category\n",
        "        # since the subcategory values have been ordered by category\n",
        "        self.subcategory_vector = None\n",
        "        # in this case we can assume that we have only two types (entity vs concept)\n",
        "        self.type_vector = type_vector(base.type)\n",
        "        self.desc_glove_vector = None\n",
        "        self.output_label = label_to_number(base.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.base_entity + \" < \" + str(len(self.desc_text)) + \", \" + str(len(self.wiki_text)) + \" >\"\n",
        "\n",
        "    def dataset_item(self):\n",
        "        return {\n",
        "            \"desc\" : self.desc_vector,\n",
        "            \"wiki\" : self.wiki_vector,\n",
        "            \"labels\" : self.labels_vector,\n",
        "            \"descriptions\" : self.descriptions_vector,\n",
        "            \"aliases\" : self.aliases_vector,\n",
        "            \"pages\" : self.pages_vector,\n",
        "            \"claims\" : self.claims_vector,\n",
        "            \"category\" : self.subcategory_vector,\n",
        "            \"type\" : self.type_vector,\n",
        "            \"desc_glove\" : self.desc_glove_vector,\n",
        "            \"output_label\" : self.output_label,\n",
        "            \"base\" : self.base_entity\n",
        "        }\n",
        "\n",
        "class CategoryTable:\n",
        "    def __init__(self):\n",
        "        self.subcategories_entered = {}  # to avoid duplicates\n",
        "        self.subcategories = []\n",
        "        self.categories = []\n",
        "        self.subcategory_to_id = None  # computed on build\n",
        "\n",
        "    def include(self, processed_entity: ProcessedEntity):\n",
        "        if processed_entity.subcategory in self.subcategories_entered:\n",
        "            return\n",
        "        self.subcategories_entered[processed_entity.subcategory] = True\n",
        "        self.subcategories.append(processed_entity.subcategory)\n",
        "        self.categories.append(processed_entity.category)\n",
        "\n",
        "    def build(self):\n",
        "        data = {\n",
        "            'subcategory': self.subcategories,\n",
        "            'category': self.categories\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        df = df.sort_values('category')\n",
        "        print(df.to_markdown())\n",
        "        self.subcategory_to_id = {row[\"subcategory\"]: index for index, (_, row) in enumerate(df.iterrows())}\n",
        "\n",
        "    def subcat_to_vector(self, subcategory):\n",
        "        vector = np.zeros(1, dtype=np.float32)\n",
        "        vector[0] = vector[0] + self.subcategory_to_id[subcategory]\n",
        "        return vector\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.subcategory_to_id)\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self):\n",
        "        self.occurrences = []\n",
        "        self.unk_token = None\n",
        "        self.word_to_id = None\n",
        "\n",
        "    def include(self, tokenized_text):\n",
        "        self.occurrences.extend(tokenized_text)\n",
        "\n",
        "    def build(self, max_vocab_size, unk_token):\n",
        "        self.unk_token = unk_token\n",
        "        counter = collections.Counter(self.occurrences)\n",
        "        self.word_to_id = {key: index for index, (key, _) in enumerate(counter.most_common(max_vocab_size - 1))}\n",
        "        assert unk_token not in self.word_to_id\n",
        "        self.word_to_id[unk_token] = max_vocab_size - 1\n",
        "        self.occurrences = None # free memory space\n",
        "\n",
        "    def build_no_limits(self):\n",
        "        counter = collections.Counter(self.occurrences)\n",
        "        self.word_to_id = {key: index for index, (key, _) in enumerate(counter.most_common())}\n",
        "        self.occurrences = None # free memory space\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.word_to_id)\n",
        "\n",
        "    def words_to_vector(self, words):\n",
        "        vector = np.zeros(self.length(), dtype=np.float32)\n",
        "        for word in words:\n",
        "            if word == self.unk_token:\n",
        "                continue\n",
        "            if word in self.word_to_id:\n",
        "                vector[self.word_to_id[word]] = vector[self.word_to_id[word]] + 1\n",
        "        return vector\n",
        "\n",
        "    # Using map to denote a Python dictionary,\n",
        "    # since the dictionary is already use for a word (term) dictionary\n",
        "    def map_to_vector(self, dictionary):\n",
        "        vector = np.zeros(self.length(), dtype=np.float32)\n",
        "        for word, freq in dictionary.items():\n",
        "            if word == self.unk_token:\n",
        "                continue\n",
        "            if word in self.word_to_id:\n",
        "                vector[self.word_to_id[word]] = freq\n",
        "        return vector\n",
        "\n",
        "PAD_TOKEN = '<PAD>'\n",
        "\n",
        "class GloveProcessing:\n",
        "    def __init__(self, context_size):\n",
        "        self.glove = GloVe(name='6B', dim=100)\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def words_to_vect(self, words):\n",
        "        return self.glove.get_vecs_by_tokens(self.tokens(words)).view(-1)\n",
        "\n",
        "    def tokens(self, words):\n",
        "        return words[:self.context_size] + [PAD_TOKEN]*(self.context_size-len(words))\n",
        "\n",
        "UNK = '<UNK>' # the token to be used for out of vocabulary words\n",
        "DESC_VOCAB_SIZE = 4_000\n",
        "WIKI_VOCAB_SIZE = 10_000\n",
        "CLAIM_VOCAB_SIZE = 500\n",
        "GLOVE_EMBEDDING_SIZE = 20\n",
        "\n",
        "class Dictionaries:\n",
        "    def __init__(self):\n",
        "        # descriptions and wiki text words are in 2 different vector spaces\n",
        "        self.desc = Dictionary()\n",
        "        self.wiki = Dictionary()\n",
        "        # we use the same languages keys dictionaries for:\n",
        "        # labels_text, descriptions_text, aliases_text, pages_text\n",
        "        self.languages = Dictionary()\n",
        "        self.claims = Dictionary()\n",
        "        self.category_table = CategoryTable()\n",
        "        # extra add glove embeddings\n",
        "        self.glove_desc = GloveProcessing(GLOVE_EMBEDDING_SIZE)\n",
        "\n",
        "    def include(self, processed_entity: ProcessedEntity):\n",
        "        self.desc.include(processed_entity.desc_text)\n",
        "        self.wiki.include(processed_entity.wiki_text)\n",
        "        self.languages.include(processed_entity.labels_text)\n",
        "        self.languages.include(processed_entity.descriptions_text)\n",
        "        self.languages.include(processed_entity.aliases_text)\n",
        "        self.languages.include(processed_entity.pages_text)\n",
        "        self.claims.include(list(processed_entity.claims_map.keys()))\n",
        "        self.category_table.include(processed_entity)\n",
        "\n",
        "    def build(self):\n",
        "        self.desc.build(DESC_VOCAB_SIZE, UNK)\n",
        "        self.wiki.build(WIKI_VOCAB_SIZE, UNK)\n",
        "        self.claims.build(CLAIM_VOCAB_SIZE, UNK)\n",
        "        # those guys are not too large: so we can not limit them\n",
        "        self.languages.build_no_limits()\n",
        "        self.category_table.build()\n",
        "\n",
        "    def finalize(self, processed_entity: ProcessedEntity):\n",
        "        processed_entity.desc_vector = self.desc.words_to_vector(processed_entity.desc_text)\n",
        "        processed_entity.wiki_vector = self.wiki.words_to_vector(processed_entity.wiki_text)\n",
        "        processed_entity.labels_vector = self.languages.words_to_vector(processed_entity.labels_text)\n",
        "        processed_entity.descriptions_vector = self.languages.words_to_vector(processed_entity.descriptions_text)\n",
        "        processed_entity.aliases_vector = self.languages.words_to_vector(processed_entity.aliases_text)\n",
        "        processed_entity.pages_vector = self.languages.words_to_vector(processed_entity.pages_text)\n",
        "        processed_entity.claims_vector = self.claims.map_to_vector(processed_entity.claims_map)\n",
        "        processed_entity.subcategory_vector = self.category_table.subcat_to_vector(processed_entity.subcategory)\n",
        "        processed_entity.desc_glove_vector = self.glove_desc.words_to_vect(processed_entity.desc_text)\n",
        "\n",
        "class IterableEntities(IterableDataset):\n",
        "    def __init__(self, processed_entities: list[ProcessedEntity]):\n",
        "        self.processed_entities = processed_entities\n",
        "\n",
        "    def __iter__(self):\n",
        "        for entity in self.processed_entities:\n",
        "            yield entity.dataset_item()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_entities)\n",
        "\n",
        "TRAINING_PROC_FILE_NAME = \"training-proc.bin\"\n",
        "VALIDATION_PROC_FILE_NAME = \"validation-proc.bin\"\n",
        "\n",
        "def text_process(text, stop):\n",
        "    result = []\n",
        "    if text is None:\n",
        "        return result\n",
        "    for sentence in nltk.sent_tokenize(text.lower()):\n",
        "        result.extend([WordNetLemmatizer().lemmatize(i) for i in nltk.word_tokenize(sentence) if i not in stop])\n",
        "    return result\n",
        "\n",
        "def create_processed(entity, dictionaries, stop):\n",
        "    description_tokenized = text_process(entity.description, stop)\n",
        "    wiki_text_tokenized = text_process(entity.wiki_text, stop)\n",
        "    result = ProcessedEntity(entity, description_tokenized, wiki_text_tokenized)\n",
        "    dictionaries.include(result)\n",
        "    return result\n",
        "\n",
        "class ProcessedDataset(NLPDataset):\n",
        "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
        "        super().__init__(training_limit, validation_limit, force_reload)\n",
        "        if not (os.path.exists(TRAINING_PROC_FILE_NAME)) or not (os.path.exists(VALIDATION_PROC_FILE_NAME)) or force_reload:\n",
        "            self.processed_training_set, self.processed_validation_set = self.processing()\n",
        "            dump(TRAINING_PROC_FILE_NAME, self.processed_training_set)\n",
        "            dump(VALIDATION_PROC_FILE_NAME, self.processed_validation_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.processed_training_set = load(TRAINING_PROC_FILE_NAME)\n",
        "            self.processed_validation_set = load(VALIDATION_PROC_FILE_NAME)\n",
        "\n",
        "    def processing(self):\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('punkt_tab')\n",
        "        stop = set(stopwords.words('english') + list(string.punctuation) + ['==', \"''\", '``', \"'s\", '==='])\n",
        "        print(\"processing the data\")\n",
        "        dictionaries = Dictionaries()\n",
        "        # from the base data, add a list of processed entities\n",
        "        print(\"training set text processing started\")\n",
        "        processed_training_set = []\n",
        "        for index, entity in enumerate(self.training_set):\n",
        "            processed_training_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"training set processed\", index+1, \"entities\")\n",
        "        print(\"training set text processing ended\")\n",
        "        print(\"validation set text processing started\")\n",
        "        processed_validation_set = []\n",
        "        for index, entity in enumerate(self.validation_set):\n",
        "            processed_validation_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"validation set processed\", index+1, \"entities\")\n",
        "        print(\"validation set text processing ended\")\n",
        "        print(\"building dictionaries\")\n",
        "        # when we've collected all the words for the two spaces, we can build them\n",
        "        dictionaries.build()\n",
        "        print(\"text to vector started\")\n",
        "        # build the vectors from the texts\n",
        "        for entity in processed_training_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        for entity in processed_validation_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        print(\"text to vector finished\")\n",
        "        return processed_training_set, processed_validation_set\n",
        "\n",
        "    def training(self):\n",
        "        return IterableEntities(self.processed_training_set)\n",
        "\n",
        "    def validation(self):\n",
        "        return IterableEntities(self.processed_validation_set)"
      ],
      "metadata": {
        "id": "cLFV7M3iwrKf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = ProcessedDataset()"
      ],
      "metadata": {
        "id": "Er0coBDiQbom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, csv\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from huggingface_hub import PyTorchModelHubMixin"
      ],
      "metadata": {
        "id": "dKeySnraTKD3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_vector_layer(params):\n",
        "    in_features, out_features = params\n",
        "    # frequency vector fields rescaling (applying also a RuLU individually):\n",
        "    return nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
        "\n",
        "class MultiModalModel(nn.Module, PyTorchModelHubMixin,\n",
        "                      repo_url=\"fax4ever/culturalitems-no-transformer\",\n",
        "                      pipeline_tag=\"text-classification\",\n",
        "                      license=\"apache-2.0\"):\n",
        "    def __init__(self, params, device) -> None:\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.device = device\n",
        "        # individual input layers for frequency vectors\n",
        "        self.desc = rescale_vector_layer(params['desc']).to(device)\n",
        "        self.wiki = rescale_vector_layer(params['wiki']).to(device)\n",
        "        self.labels = rescale_vector_layer(params['labels']).to(device)\n",
        "        self.descriptions = rescale_vector_layer(params['descriptions']).to(device)\n",
        "        self.aliases = rescale_vector_layer(params['aliases']).to(device)\n",
        "        self.pages = rescale_vector_layer(params['pages']).to(device)\n",
        "        self.claims = rescale_vector_layer(params['claims']).to(device)\n",
        "        # individual input layers for scalar value\n",
        "        self.category = nn.Linear(params['category_dim'], params['category_scale']).to(device)\n",
        "        self.type_proj = nn.Linear(params['type_dim'], params['type_scale']).to(device)\n",
        "        # glove\n",
        "        self.desc_glove = rescale_vector_layer(params['desc_glove']).to(device)\n",
        "        # common classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(params['total_scale'], params['hidden_layers']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(params['dropout']),\n",
        "            nn.Linear(params['hidden_layers'], 3)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, dataset_items):\n",
        "        desc_feat = self.desc(dataset_items['desc'].to(self.device))\n",
        "        wiki_feat = self.wiki(dataset_items['wiki'].to(self.device))\n",
        "        labels_feat = self.labels(dataset_items['labels'].to(self.device))\n",
        "        descriptions_feat = self.descriptions(dataset_items['descriptions'].to(self.device))\n",
        "        aliases_feat = self.aliases(dataset_items['aliases'].to(self.device))\n",
        "        pages_feat = self.pages(dataset_items['pages'].to(self.device))\n",
        "        claims_feat = self.claims(dataset_items['claims'].to(self.device))\n",
        "        category_feat = self.category(dataset_items['category'].to(self.device))\n",
        "        type_feat = self.type_proj(dataset_items['type'].to(self.device))\n",
        "        desc_glove_feat = self.desc_glove(dataset_items['desc_glove'].to(self.device))\n",
        "        combined = torch.cat([desc_feat, desc_glove_feat, wiki_feat, labels_feat, descriptions_feat, aliases_feat, pages_feat,\n",
        "                              claims_feat, category_feat, type_feat], dim=1)\n",
        "        return self.classifier(combined)"
      ],
      "metadata": {
        "id": "ZRIukpPuTOAF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_label(label):\n",
        "    if label == 0:\n",
        "        return 'cultural agnostic'\n",
        "    if label == 1:\n",
        "        return 'cultural representative'\n",
        "    if label == 2:\n",
        "        return 'cultural exclusive'\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "model = MultiModalModel.from_pretrained(\"fax4ever/culturalitems-no-transformer\")\n",
        "matching = 0\n",
        "\n",
        "with open('no-transformer-inference.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    field = [\"entity\", \"true label\", \"prediction\", \"correct\"]\n",
        "    writer.writerow(field)\n",
        "    with torch.no_grad():\n",
        "        validation = ProcessedDataset().validation()\n",
        "        for entity in DataLoader(validation):\n",
        "            prediction = model(entity).detach().clone().argmax(dim=1).numpy()[0]\n",
        "            true_label = entity['output_label'].numpy()[0]\n",
        "            match = prediction == true_label\n",
        "            if match:\n",
        "                matching = matching + 1\n",
        "            base_ = entity['base'][0]\n",
        "            writer.writerow([base_, number_to_label(true_label), number_to_label(prediction), match])\n",
        "\n",
        "print('matched', matching, 'on', len(validation), '(', matching/len(validation), ')')"
      ],
      "metadata": {
        "id": "c8AsFndzTknl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}