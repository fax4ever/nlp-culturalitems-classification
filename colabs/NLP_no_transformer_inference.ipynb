{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP no-Transformer: Inference"
      ],
      "metadata": {
        "id": "JxYfp7T4WsdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikidata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "cbz6lsIPctHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885a6630-7e17-43d8-8c28-78e74bfd89b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Wikidata in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2BXVShMsA0-4"
      },
      "outputs": [],
      "source": [
        "import torch, random, requests, os, pickle\n",
        "import numpy as np\n",
        "from wikidata.client import Client\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting seeds to try to make the training as much deterministic as possible.\n",
        "This should help to compare results (for the instance accuracy of the validation test) of different trainings."
      ],
      "metadata": {
        "id": "lVNhtixBXB6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "lEcH5MUeGVTR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia pages and Wikidata data need to be loaded from the web.\n",
        "In order to speed up the training and the inference, we store (cache) the retrieved and the processed data in files."
      ],
      "metadata": {
        "id": "HiF9zqMCXIpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "BASE_PATH = '/content/drive/MyDrive/Lost_in_Language_Recognition/'\n",
        "\n",
        "def dump(file_name, result):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    # remove dump files if present\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "    with open(file_path, 'wb') as file:\n",
        "        print(\"dumping\", file_path)\n",
        "        # noinspection PyTypeChecker\n",
        "        pickle.dump(result, file)\n",
        "\n",
        "def load(file_name):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    with open(file_path, 'rb') as file:\n",
        "        print(\"loading\", file_path)\n",
        "        # noinspection PyTypeChecker\n",
        "        return pickle.load(file)\n",
        "\n",
        "def file_exists(file_name):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    return os.path.exists(file_path)"
      ],
      "metadata": {
        "id": "xCas4n7JePkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2763f95-c0c5-4f46-905d-1c281ccf8b56"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we build the singleton `NLPDataset`, that contains:\n",
        "1. The original Hugging Face dataset\n",
        "2. The Wikidata entities\n",
        "3. The Wikipedia pages"
      ],
      "metadata": {
        "id": "EoF91ED2XtJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia_pages(sitelinks):\n",
        "    result = []\n",
        "    for site_key in sitelinks.keys():\n",
        "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "            lang = site_key.replace(\"wiki\", \"\")\n",
        "            result.append(lang)\n",
        "    return result\n",
        "\n",
        "def build_claims(claims):\n",
        "    result = {}\n",
        "    for prop_id, values in claims.items():\n",
        "        result[prop_id] = len(values)\n",
        "    return result\n",
        "\n",
        "class Entity:\n",
        "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
        "        self.entity_id = entity_id\n",
        "        if 'label' in dataset_item:\n",
        "            self.label = dataset_item['label']\n",
        "        self.name = dataset_item['name']\n",
        "        self.description = dataset_item['description']\n",
        "        self.type = dataset_item['type']\n",
        "        self.category = dataset_item['category']\n",
        "        self.subcategory = dataset_item['subcategory']\n",
        "        self.wiki_text = wiki_text\n",
        "        # Languages\n",
        "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
        "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
        "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
        "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
        "        # Properties\n",
        "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
        "\n",
        "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wiki_text(en_wiki):\n",
        "    if not en_wiki:\n",
        "        return None\n",
        "    title = en_wiki[\"title\"]\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
        "    res = requests.get(API_URL, params=params, headers=headers)\n",
        "    json = res.json()\n",
        "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
        "    # Keep the original text as it is.\n",
        "    # The text will be processed in other methods,\n",
        "    # such as processed_dataset#tokenize().\n",
        "    return page.get(\"extract\", \"\")\n",
        "\n",
        "class EntityFactory:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, item):\n",
        "        entity_id = extract_entity_id(item['item'])\n",
        "        try:\n",
        "            wikidata = self.client.get(entity_id, load=True)\n",
        "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
        "            en_wiki = sitelinks.get(\"enwiki\")\n",
        "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
        "        except Exception as e:\n",
        "            print(\"Error loading id:\", entity_id, e)\n",
        "            return None\n",
        "\n",
        "TEST_SET_FILE_NAME = BASE_PATH + \"test_unlabeled.csv\"\n",
        "\n",
        "TRAINING_FILE_NAME = \"training.bin\"\n",
        "VALIDATION_FILE_NAME = \"validation.bin\"\n",
        "TEST_FILE_NAME = \"test.bin\"\n",
        "\n",
        "def create_set(dataset, factory, file_name):\n",
        "    limit = len(dataset)\n",
        "    result = []\n",
        "    for index, item in enumerate(dataset):\n",
        "        created = factory.create(item)\n",
        "        if created is not None:\n",
        "            result.append(created)\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
        "    return result\n",
        "\n",
        "def load_or_create_set(factory, dataset, file_name):\n",
        "    if not (file_exists(file_name)):\n",
        "        created = create_set(dataset, factory, file_name)\n",
        "        dump(file_name, created)\n",
        "        return created\n",
        "    else:\n",
        "        return load(file_name)\n",
        "\n",
        "class NLPDataset:\n",
        "    def __init__(self):\n",
        "        if (not (file_exists(TRAINING_FILE_NAME)) or not (file_exists(VALIDATION_FILE_NAME))\n",
        "                or not (file_exists(TEST_FILE_NAME))):\n",
        "            # load the project dataset\n",
        "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HF_TOKEN'))\n",
        "            # a factory object is used to create our entities\n",
        "            factory = EntityFactory(Client())\n",
        "\n",
        "            self.training_set = load_or_create_set(factory, dataset['train'], TRAINING_FILE_NAME)\n",
        "            self.validation_set = load_or_create_set(factory, dataset['validation'], VALIDATION_FILE_NAME)\n",
        "            self.test_set = load_or_create_set(factory, pd.read_csv(TEST_SET_FILE_NAME).to_dict('records'), TEST_FILE_NAME)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.training_set = load(TRAINING_FILE_NAME)\n",
        "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
        "            self.test_set = load(TEST_FILE_NAME)\n",
        "\n",
        "    def __str__(self):\n",
        "        return (\"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set)) +\n",
        "                \". test: \" + str(len(self.test_set)))\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set))"
      ],
      "metadata": {
        "id": "jaK4aqHbMhFs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_dataset = NLPDataset()"
      ],
      "metadata": {
        "id": "2oVOAmBrhwPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26300a16-24b1-4d89-f438-cce18c5d0f5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/training.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/validation.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/test.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dump files `training.bin` and `validation.bin` are present, the instance is build from the dump.\n",
        "Without transformers we're going to process manually fields and texts.\n",
        "In the following code:\n",
        "1. dictionaries are created to produce frequency vectors\n",
        "2. GloVe pretrained embeddings are applied to the descriptions\n",
        "3. the labels are mapped to numbers\n",
        "4. Category and subcategories are mapped to a single scalar value for each entity - pandas is used to group the ids of the subcategories by the category\n",
        "\n",
        "The result is a singleton `ProcessedDataset` that extends the base `NLPDataset`\n",
        "providing also the `ProcessedEntity`s of the corresponding (base) `Entity`.\n",
        "\n",
        "Finally the result of the processing is dumped and can be reused to the next run\n"
      ],
      "metadata": {
        "id": "deGmBQyhX0UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "metadata": {
        "id": "MxmjT68Nxt2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8312fc-73e5-482e-9575-998fefb83f15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchtext==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchtext==0.6.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections, nltk, string\n",
        "import pandas as pd\n",
        "from torchtext.vocab import GloVe\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from torch.utils.data import IterableDataset"
      ],
      "metadata": {
        "id": "eR3AbdkjxJRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def type_vector(base_type):\n",
        "    vector = np.zeros(1, dtype=np.float32)\n",
        "    if base_type == 'entity':\n",
        "        vector[0] = vector[0] + 1\n",
        "    return vector\n",
        "\n",
        "def label_to_number(label):\n",
        "    if label == 'cultural agnostic':\n",
        "        return 0\n",
        "    if label == 'cultural representative':\n",
        "        return 1\n",
        "    if label == 'cultural exclusive':\n",
        "        return 2\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "class ProcessedEntity:\n",
        "    def __init__(self, base: Entity, desc_text, wiki_text):\n",
        "        self.entity_id = base.entity_id\n",
        "        self.name = base.name\n",
        "        # processed fields\n",
        "        self.desc_text = desc_text\n",
        "        self.wiki_text = wiki_text\n",
        "        self.labels_text = base.labels\n",
        "        self.descriptions_text = base.descriptions\n",
        "        self.aliases_text = base.aliases\n",
        "        self.pages_text = base.wikipedia_pages\n",
        "        # Using map to denote a Python dictionary,\n",
        "        # since the dictionary is already use for a word (term) dictionary\n",
        "        self.claims_map = base.claims\n",
        "        self.subcategory = base.subcategory\n",
        "        self.category = base.category\n",
        "\n",
        "        # build later (then the dictionaries are finalized)\n",
        "        self.desc_vector = None\n",
        "        self.wiki_vector = None\n",
        "        self.labels_vector = None\n",
        "        self.descriptions_vector = None\n",
        "        self.aliases_vector = None\n",
        "        self.pages_vector = None\n",
        "        self.claims_vector = None\n",
        "        # it includes implicitly the category\n",
        "        # since the subcategory values have been ordered by category\n",
        "        self.subcategory_vector = None\n",
        "        # in this case we can assume that we have only two types (entity vs concept)\n",
        "        self.type_vector = type_vector(base.type)\n",
        "        self.desc_glove_vector = None\n",
        "        if hasattr(base, 'label'):\n",
        "            self.output_label = label_to_number(base.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.name + \" < \" + str(len(self.desc_text)) + \", \" + str(len(self.wiki_text)) + \" >\"\n",
        "\n",
        "    def dataset_item(self):\n",
        "        return {\n",
        "            \"desc\" : self.desc_vector,\n",
        "            \"wiki\" : self.wiki_vector,\n",
        "            \"labels\" : self.labels_vector,\n",
        "            \"descriptions\" : self.descriptions_vector,\n",
        "            \"aliases\" : self.aliases_vector,\n",
        "            \"pages\" : self.pages_vector,\n",
        "            \"claims\" : self.claims_vector,\n",
        "            \"category\" : self.subcategory_vector,\n",
        "            \"type\" : self.type_vector,\n",
        "            \"desc_glove\" : self.desc_glove_vector,\n",
        "            \"output_label\" : self.output_label if hasattr(self, 'output_label') else 0,\n",
        "            \"entity_id\" : self.entity_id,\n",
        "            \"name\" : self.name\n",
        "        }\n",
        "\n",
        "class CategoryTable:\n",
        "    def __init__(self):\n",
        "        self.subcategories_entered = {}  # to avoid duplicates\n",
        "        self.subcategories = []\n",
        "        self.categories = []\n",
        "        self.subcategory_to_id = None  # computed on build\n",
        "\n",
        "    def include(self, processed_entity: ProcessedEntity):\n",
        "        if processed_entity.subcategory in self.subcategories_entered:\n",
        "            return\n",
        "        self.subcategories_entered[processed_entity.subcategory] = True\n",
        "        self.subcategories.append(processed_entity.subcategory)\n",
        "        self.categories.append(processed_entity.category)\n",
        "\n",
        "    def build(self):\n",
        "        data = {\n",
        "            'subcategory': self.subcategories,\n",
        "            'category': self.categories\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "        df = df.sort_values('category')\n",
        "        print(df.to_markdown())\n",
        "        self.subcategory_to_id = {row[\"subcategory\"]: index for index, (_, row) in enumerate(df.iterrows())}\n",
        "\n",
        "    def subcat_to_vector(self, subcategory):\n",
        "        vector = np.zeros(1, dtype=np.float32)\n",
        "        vector[0] = vector[0] + self.subcategory_to_id[subcategory]\n",
        "        return vector\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.subcategory_to_id)\n",
        "\n",
        "class Dictionary:\n",
        "    def __init__(self):\n",
        "        self.occurrences = []\n",
        "        self.unk_token = None\n",
        "        self.word_to_id = None\n",
        "\n",
        "    def include(self, tokenized_text):\n",
        "        self.occurrences.extend(tokenized_text)\n",
        "\n",
        "    def build(self, max_vocab_size, unk_token):\n",
        "        self.unk_token = unk_token\n",
        "        counter = collections.Counter(self.occurrences)\n",
        "        self.word_to_id = {key: index for index, (key, _) in enumerate(counter.most_common(max_vocab_size - 1))}\n",
        "        assert unk_token not in self.word_to_id\n",
        "        self.word_to_id[unk_token] = max_vocab_size - 1\n",
        "        self.occurrences = None # free memory space\n",
        "\n",
        "    def build_no_limits(self):\n",
        "        counter = collections.Counter(self.occurrences)\n",
        "        self.word_to_id = {key: index for index, (key, _) in enumerate(counter.most_common())}\n",
        "        self.occurrences = None # free memory space\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.word_to_id)\n",
        "\n",
        "    def words_to_vector(self, words):\n",
        "        vector = np.zeros(self.length(), dtype=np.float32)\n",
        "        for word in words:\n",
        "            if word == self.unk_token:\n",
        "                continue\n",
        "            if word in self.word_to_id:\n",
        "                vector[self.word_to_id[word]] = vector[self.word_to_id[word]] + 1\n",
        "        return vector\n",
        "\n",
        "    # Using map to denote a Python dictionary,\n",
        "    # since the dictionary is already use for a word (term) dictionary\n",
        "    def map_to_vector(self, dictionary):\n",
        "        vector = np.zeros(self.length(), dtype=np.float32)\n",
        "        for word, freq in dictionary.items():\n",
        "            if word == self.unk_token:\n",
        "                continue\n",
        "            if word in self.word_to_id:\n",
        "                vector[self.word_to_id[word]] = freq\n",
        "        return vector\n",
        "\n",
        "PAD_TOKEN = '<PAD>'\n",
        "\n",
        "class GloveProcessing:\n",
        "    def __init__(self, context_size):\n",
        "        self.glove = GloVe(name='6B', dim=100)\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def words_to_vect(self, words):\n",
        "        return self.glove.get_vecs_by_tokens(self.tokens(words)).view(-1)\n",
        "\n",
        "    def tokens(self, words):\n",
        "        return words[:self.context_size] + [PAD_TOKEN]*(self.context_size-len(words))\n",
        "\n",
        "UNK = '<UNK>' # the token to be used for out of vocabulary words\n",
        "DESC_VOCAB_SIZE = 4_000\n",
        "WIKI_VOCAB_SIZE = 10_000\n",
        "GLOVE_EMBEDDING_SIZE = 20\n",
        "\n",
        "class Dictionaries:\n",
        "    def __init__(self):\n",
        "        # descriptions and wiki text words are in 2 different vector spaces\n",
        "        self.desc = Dictionary()\n",
        "        self.wiki = Dictionary()\n",
        "        # we use the same languages keys dictionaries for:\n",
        "        # labels_text, descriptions_text, aliases_text, pages_text\n",
        "        self.languages = Dictionary()\n",
        "        self.claims = Dictionary()\n",
        "        self.category_table = CategoryTable()\n",
        "        # extra add glove embeddings\n",
        "        self.glove_desc = GloveProcessing(GLOVE_EMBEDDING_SIZE)\n",
        "\n",
        "    def include(self, processed_entity: ProcessedEntity):\n",
        "        self.desc.include(processed_entity.desc_text)\n",
        "        self.wiki.include(processed_entity.wiki_text)\n",
        "        self.languages.include(processed_entity.labels_text)\n",
        "        self.languages.include(processed_entity.descriptions_text)\n",
        "        self.languages.include(processed_entity.aliases_text)\n",
        "        self.languages.include(processed_entity.pages_text)\n",
        "        self.claims.include(list(processed_entity.claims_map.keys()))\n",
        "        self.category_table.include(processed_entity)\n",
        "\n",
        "    def build(self):\n",
        "        self.desc.build(DESC_VOCAB_SIZE, UNK)\n",
        "        self.wiki.build(WIKI_VOCAB_SIZE, UNK)\n",
        "        self.claims.build_no_limits()\n",
        "        # those guys are not too large: so we can not limit them\n",
        "        self.languages.build_no_limits()\n",
        "        self.category_table.build()\n",
        "\n",
        "    def finalize(self, processed_entity: ProcessedEntity):\n",
        "        processed_entity.desc_vector = self.desc.words_to_vector(processed_entity.desc_text)\n",
        "        processed_entity.wiki_vector = self.wiki.words_to_vector(processed_entity.wiki_text)\n",
        "        processed_entity.labels_vector = self.languages.words_to_vector(processed_entity.labels_text)\n",
        "        processed_entity.descriptions_vector = self.languages.words_to_vector(processed_entity.descriptions_text)\n",
        "        processed_entity.aliases_vector = self.languages.words_to_vector(processed_entity.aliases_text)\n",
        "        processed_entity.pages_vector = self.languages.words_to_vector(processed_entity.pages_text)\n",
        "        processed_entity.claims_vector = self.claims.map_to_vector(processed_entity.claims_map)\n",
        "        processed_entity.subcategory_vector = self.category_table.subcat_to_vector(processed_entity.subcategory)\n",
        "        processed_entity.desc_glove_vector = self.glove_desc.words_to_vect(processed_entity.desc_text)\n",
        "\n",
        "class IterableEntities(IterableDataset):\n",
        "    def __init__(self, processed_entities: list[ProcessedEntity]):\n",
        "        self.processed_entities = processed_entities\n",
        "\n",
        "    def __iter__(self):\n",
        "        for entity in self.processed_entities:\n",
        "            yield entity.dataset_item()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_entities)\n",
        "\n",
        "TRAINING_PROC_FILE_NAME = \"training-proc.bin\"\n",
        "VALIDATION_PROC_FILE_NAME = \"validation-proc.bin\"\n",
        "TEST_PROC_FILE_NAME = \"test-proc.bin\"\n",
        "\n",
        "def text_process(text, stop):\n",
        "    result = []\n",
        "    if text is None:\n",
        "        return result\n",
        "    for sentence in nltk.sent_tokenize(text.lower()):\n",
        "        result.extend([WordNetLemmatizer().lemmatize(i) for i in nltk.word_tokenize(sentence) if i not in stop])\n",
        "    return result\n",
        "\n",
        "def create_processed(entity, dictionaries, stop):\n",
        "    description_tokenized = text_process(entity.description, stop)\n",
        "    wiki_text_tokenized = text_process(entity.wiki_text, stop)\n",
        "    result = ProcessedEntity(entity, description_tokenized, wiki_text_tokenized)\n",
        "    dictionaries.include(result)\n",
        "    return result\n",
        "\n",
        "class ProcessedDataset(NLPDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        if not (file_exists(TRAINING_PROC_FILE_NAME)) or not (file_exists(VALIDATION_PROC_FILE_NAME)) or not (file_exists(TEST_PROC_FILE_NAME)):\n",
        "            self.processed_training_set, self.processed_validation_set, self.processed_test_set = self.processing()\n",
        "            dump(TRAINING_PROC_FILE_NAME, self.processed_training_set)\n",
        "            dump(VALIDATION_PROC_FILE_NAME, self.processed_validation_set)\n",
        "            dump(TEST_PROC_FILE_NAME, self.processed_test_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.processed_training_set = load(TRAINING_PROC_FILE_NAME)\n",
        "            self.processed_validation_set = load(VALIDATION_PROC_FILE_NAME)\n",
        "            self.processed_test_set = load(TEST_PROC_FILE_NAME)\n",
        "\n",
        "    def processing(self):\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('punkt_tab')\n",
        "        stop = set(stopwords.words('english') + list(string.punctuation) + ['==', \"''\", '``', \"'s\", '==='])\n",
        "        print(\"processing the data\")\n",
        "        dictionaries = Dictionaries()\n",
        "        # from the base data, add a list of processed entities\n",
        "        print(\"training set text processing started\")\n",
        "        processed_training_set = []\n",
        "        for index, entity in enumerate(self.training_set):\n",
        "            processed_training_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"training set processed\", index+1, \"entities\")\n",
        "        print(\"training set text processing ended\")\n",
        "        print(\"validation set text processing started\")\n",
        "        processed_validation_set = []\n",
        "        for index, entity in enumerate(self.validation_set):\n",
        "            processed_validation_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"validation set processed\", index+1, \"entities\")\n",
        "        print(\"validation set text processing ended\")\n",
        "        print(\"test set text processing started\")\n",
        "        processed_test_set = []\n",
        "        for index, entity in enumerate(self.test_set):\n",
        "            processed_test_set.append(create_processed(entity, dictionaries, stop))\n",
        "            if (index+1) % 100 == 0:\n",
        "                print(\"test set processed\", index+1, \"entities\")\n",
        "        print(\"test set text processing ended\")\n",
        "        print(\"building dictionaries\")\n",
        "        # when we've collected all the words for the two spaces, we can build them\n",
        "        dictionaries.build()\n",
        "        print(\"text to vector started\")\n",
        "        # build the vectors from the texts\n",
        "        for entity in processed_training_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        for entity in processed_validation_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        for entity in processed_test_set:\n",
        "            dictionaries.finalize(entity)\n",
        "        print(\"text to vector finished\")\n",
        "        return processed_training_set, processed_validation_set, processed_test_set\n",
        "\n",
        "    def training(self):\n",
        "        return IterableEntities(self.processed_training_set)\n",
        "\n",
        "    def validation(self):\n",
        "        return IterableEntities(self.processed_validation_set)\n",
        "\n",
        "    def test(self):\n",
        "        return IterableEntities(self.processed_test_set)"
      ],
      "metadata": {
        "id": "cLFV7M3iwrKf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = ProcessedDataset()"
      ],
      "metadata": {
        "id": "Er0coBDiQbom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8df7b0-f4b9-4d2d-8e32-a48ad70c8758"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/training.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/validation.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/test.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/training-proc.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/validation-proc.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/test-proc.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, csv\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from huggingface_hub import PyTorchModelHubMixin"
      ],
      "metadata": {
        "id": "dKeySnraTKD3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arrived at the very heart of the No-Trasformer solution: the model!\n",
        "We want to use all the data source from the dataset, the WikiData and WikiPedia pages. Since those guys have a very different original dimentions we the code idea is to scale them in a pre training layer before to produce the combined vector that is passed in input the classifier (a FF network)."
      ],
      "metadata": {
        "id": "p3yEmz_zZr0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale_vector_layer(params):\n",
        "    in_features, out_features = params\n",
        "    # frequency vector fields rescaling (applying also a RuLU individually):\n",
        "    return nn.Sequential(nn.Linear(in_features, out_features), nn.ReLU())\n",
        "\n",
        "class MultiModalModel(nn.Module, PyTorchModelHubMixin,\n",
        "                      repo_url=\"fax4ever/culturalitems-no-transformer\",\n",
        "                      pipeline_tag=\"text-classification\",\n",
        "                      license=\"apache-2.0\"):\n",
        "    def __init__(self, params, device) -> None:\n",
        "        super(MultiModalModel, self).__init__()\n",
        "        self.device = device\n",
        "        # individual input layers for frequency vectors\n",
        "        self.desc = rescale_vector_layer(params['desc']).to(device)\n",
        "        self.wiki = rescale_vector_layer(params['wiki']).to(device)\n",
        "        self.labels = rescale_vector_layer(params['labels']).to(device)\n",
        "        self.descriptions = rescale_vector_layer(params['descriptions']).to(device)\n",
        "        self.aliases = rescale_vector_layer(params['aliases']).to(device)\n",
        "        self.pages = rescale_vector_layer(params['pages']).to(device)\n",
        "        self.claims = rescale_vector_layer(params['claims']).to(device)\n",
        "        # individual input layers for scalar value\n",
        "        self.category = nn.Linear(params['category_dim'], params['category_scale']).to(device)\n",
        "        self.type_proj = nn.Linear(params['type_dim'], params['type_scale']).to(device)\n",
        "        # glove\n",
        "        self.desc_glove = rescale_vector_layer(params['desc_glove']).to(device)\n",
        "        # common classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(params['total_scale'], params['hidden_layers']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(params['dropout']),\n",
        "            nn.Linear(params['hidden_layers'], 3)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, dataset_items):\n",
        "        desc_feat = self.desc(dataset_items['desc'].to(self.device))\n",
        "        wiki_feat = self.wiki(dataset_items['wiki'].to(self.device))\n",
        "        labels_feat = self.labels(dataset_items['labels'].to(self.device))\n",
        "        descriptions_feat = self.descriptions(dataset_items['descriptions'].to(self.device))\n",
        "        aliases_feat = self.aliases(dataset_items['aliases'].to(self.device))\n",
        "        pages_feat = self.pages(dataset_items['pages'].to(self.device))\n",
        "        claims_feat = self.claims(dataset_items['claims'].to(self.device))\n",
        "        category_feat = self.category(dataset_items['category'].to(self.device))\n",
        "        type_feat = self.type_proj(dataset_items['type'].to(self.device))\n",
        "        desc_glove_feat = self.desc_glove(dataset_items['desc_glove'].to(self.device))\n",
        "        combined = torch.cat([desc_feat, desc_glove_feat, wiki_feat, labels_feat, descriptions_feat, aliases_feat, pages_feat,\n",
        "                              claims_feat, category_feat, type_feat], dim=1)\n",
        "        return self.classifier(combined)"
      ],
      "metadata": {
        "id": "ZRIukpPuTOAF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the inference code. The output is also stored in a csv file: `transformer-inference.csv`. So that this can be inspected later."
      ],
      "metadata": {
        "id": "Q_WYVfZ-e1hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_label(label):\n",
        "    if label == 0:\n",
        "        return 'cultural agnostic'\n",
        "    if label == 1:\n",
        "        return 'cultural representative'\n",
        "    if label == 2:\n",
        "        return 'cultural exclusive'\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "model = MultiModalModel.from_pretrained(\"fax4ever/culturalitems-no-transformer\", token=userdata.get('HF_TOKEN'))\n",
        "\n",
        "matching = 0\n",
        "with torch.no_grad():\n",
        "    validation = processed_dataset.validation()\n",
        "    for entity in DataLoader(validation):\n",
        "        prediction = model(entity).detach().cpu().clone().argmax(dim=1).numpy()[0]\n",
        "        true_label = entity['output_label'].numpy()[0]\n",
        "        match = prediction == true_label\n",
        "        if match:\n",
        "            matching = matching + 1\n",
        "print('matched', matching, 'on', len(validation), '(', matching/len(validation), ')')\n",
        "\n",
        "with open(BASE_PATH + 'Lost_in_Language_Recognition_output_multimodalnn.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    field = ['item', 'name', 'label']\n",
        "    writer.writerow(field)\n",
        "    with torch.no_grad():\n",
        "        test = processed_dataset.test()\n",
        "        for entity in DataLoader(test):\n",
        "            prediction = model(entity).detach().cpu().clone().argmax(dim=1).numpy()[0]\n",
        "            writer.writerow([\"http://www.wikidata.org/entity/\" + entity['entity_id'][0], entity['name'][0], number_to_label(prediction)])"
      ],
      "metadata": {
        "id": "c8AsFndzTknl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c230c2c-ff73-4e92-a627-1c473614853e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matched 226 on 300 ( 0.7533333333333333 )\n"
          ]
        }
      ]
    }
  ]
}