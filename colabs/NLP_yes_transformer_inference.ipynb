{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikidata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DCNsUAXylqJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf356e0e-b686-4539-daa9-81bfcaffbaef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikidata\n",
            "  Downloading Wikidata-0.8.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Downloading Wikidata-0.8.1-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: Wikidata\n",
            "Successfully installed Wikidata-0.8.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random, requests, os, pickle\n",
        "import numpy as np\n",
        "from wikidata.client import Client\n",
        "from datasets import load_dataset\n",
        "from itertools import islice\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "NPYS__k7tU3Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed"
      ],
      "metadata": {
        "id": "JPOI_M3umGch"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "THzb1CV7ggEv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "c7HjzdTlmJZ-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump(file_name, result):\n",
        "    # remove dump files if present\n",
        "    if os.path.exists(file_name):\n",
        "        os.remove(file_name)\n",
        "    with open(file_name, 'wb') as file:\n",
        "        print(\"dumping\", file_name)\n",
        "        # noinspection PyTypeChecker\n",
        "        pickle.dump(result, file)\n",
        "\n",
        "def load(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        print(\"loading\", file_name)\n",
        "        # noinspection PyTypeChecker\n",
        "        return pickle.load(file)"
      ],
      "metadata": {
        "id": "xCas4n7JePkX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia_pages(sitelinks):\n",
        "    result = []\n",
        "    for site_key in sitelinks.keys():\n",
        "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "            lang = site_key.replace(\"wiki\", \"\")\n",
        "            result.append(lang)\n",
        "    return result\n",
        "\n",
        "def build_claims(claims):\n",
        "    result = {}\n",
        "    for prop_id, values in claims.items():\n",
        "        result[prop_id] = len(values)\n",
        "    return result\n",
        "\n",
        "class Entity:\n",
        "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
        "        self.entity_id = entity_id\n",
        "        self.label = dataset_item['label']\n",
        "        self.name = dataset_item['name']\n",
        "        self.description = dataset_item['description']\n",
        "        self.type = dataset_item['type']\n",
        "        self.category = dataset_item['category']\n",
        "        self.subcategory = dataset_item['subcategory']\n",
        "        self.wiki_text = wiki_text\n",
        "        # Languages\n",
        "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
        "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
        "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
        "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
        "        # Properties\n",
        "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
        "\n",
        "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wiki_text(en_wiki):\n",
        "    if not en_wiki:\n",
        "        return None\n",
        "    title = en_wiki[\"title\"]\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
        "    res = requests.get(API_URL, params=params, headers=headers)\n",
        "    json = res.json()\n",
        "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
        "    # Keep the original text as it is.\n",
        "    # The text will be processed in other methods,\n",
        "    # such as processed_dataset#tokenize().\n",
        "    return page.get(\"extract\", \"\")\n",
        "\n",
        "class EntityFactory:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, item):\n",
        "        entity_id = extract_entity_id(item['item'])\n",
        "        try:\n",
        "            wikidata = self.client.get(entity_id, load=True)\n",
        "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
        "            en_wiki = sitelinks.get(\"enwiki\")\n",
        "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
        "        except Exception as e:\n",
        "            print(\"Error loading id:\", entity_id, e)\n",
        "            return None\n",
        "\n",
        "TRAINING_FILE_NAME = \"training.bin\"\n",
        "VALIDATION_FILE_NAME = \"validation.bin\"\n",
        "\n",
        "def create_set(dataset, factory, limit, file_name):\n",
        "    # apply the limits\n",
        "    if limit is None:\n",
        "        limit = len(dataset)\n",
        "    result = []\n",
        "    for index, item in enumerate(islice(dataset, limit)):\n",
        "        created = factory.create(item)\n",
        "        if created is not None:\n",
        "            result.append(created)\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
        "    return result\n",
        "\n",
        "class NLPDataset:\n",
        "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
        "        if not (os.path.exists(TRAINING_FILE_NAME)) or not (os.path.exists(VALIDATION_FILE_NAME)) or force_reload:\n",
        "            # load the project dataset\n",
        "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HUGGINGFACE_TOKEN'))\n",
        "            # a factory object is used to create our entities\n",
        "            factory = EntityFactory(Client())\n",
        "\n",
        "            self.training_set = create_set(dataset['train'], factory, training_limit, TRAINING_FILE_NAME)\n",
        "            self.validation_set = create_set(dataset['validation'], factory, validation_limit, VALIDATION_FILE_NAME)\n",
        "            dump(TRAINING_FILE_NAME, self.training_set)\n",
        "            dump(VALIDATION_FILE_NAME, self.validation_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.training_set = load(TRAINING_FILE_NAME)\n",
        "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set))"
      ],
      "metadata": {
        "id": "MnMKuBqNkBYn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_dataset = NLPDataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4hKm7wWbmwt1",
        "outputId": "c2588ed1-e969-465e-994c-95f69d9437bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading training.bin\n",
            "loading validation.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "pUREAurO-f48"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_entity_dict():\n",
        "    entity_dict = {}\n",
        "    for entity in nlp_dataset.training_set:\n",
        "        entity_dict[entity.entity_id] = entity\n",
        "    for entity in nlp_dataset.validation_set:\n",
        "        entity_dict[entity.entity_id] = entity\n",
        "    return entity_dict\n",
        "\n",
        "def label_to_number(label):\n",
        "    if label == 'cultural agnostic':\n",
        "        return 0\n",
        "    if label == 'cultural representative':\n",
        "        return 1\n",
        "    if label == 'cultural exclusive':\n",
        "        return 2\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "class WikiDataset:\n",
        "    def __init__(self):\n",
        "        entity_dict = build_entity_dict()\n",
        "        dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HUGGINGFACE_TOKEN'))\n",
        "        # enriching the entities with the wiki pages\n",
        "        def map_labels(sample):\n",
        "            label = sample[\"label\"]\n",
        "            sample[\"label\"] = label_to_number(label)\n",
        "            wiki_id = extract_entity_id(sample[\"item\"])\n",
        "            if wiki_id is not None and wiki_id in entity_dict:\n",
        "                wiki_text = entity_dict[wiki_id].wiki_text\n",
        "                sample[\"wiki_text\"] = wiki_text if type(wiki_text) == str else \"\"\n",
        "            else:\n",
        "                sample[\"wiki_text\"] = \"\"\n",
        "            return sample\n",
        "        self.dataset = dataset.map(map_labels)\n",
        "\n",
        "    def tokenize(self, tokenizer):\n",
        "        def tokenize_function(items):\n",
        "            return tokenizer(items[\"description\"], items[\"wiki_text\"], padding=True, truncation=True)\n",
        "        return self.dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "6OETfZRXAlIo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceModel:\n",
        "    def __init__(self, repo, kind):\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(repo)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(kind)\n",
        "        self.device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def predict_text(self, desc, wiki, input_ids_ds, attention_mask_ds):\n",
        "        self.model.eval()\n",
        "        # no max length - we want to use the default of the base model\n",
        "        # as we do in training\n",
        "        encoding = self.tokenizer(desc, wiki, return_tensors='pt', padding='max_length', truncation=True)\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "        input_ids_ds = torch.tensor(input_ids_ds).to(self.device).view(1, -1)\n",
        "        attention_mask_ds = torch.tensor(attention_mask_ds).to(self.device).view(1, -1)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids_ds, attention_mask=attention_mask_ds)\n",
        "            _, prediction = torch.max(outputs.logits, dim=1)\n",
        "            outputs2 = self.model(input_ids=input_ids_ds, attention_mask=attention_mask_ds)\n",
        "            _, prediction_ds = torch.max(outputs2.logits, dim=1)\n",
        "        return prediction.item(), prediction_ds.item()"
      ],
      "metadata": {
        "id": "JJ6X4DZuBmw8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_label(label):\n",
        "    if label == 0:\n",
        "        return 'cultural agnostic'\n",
        "    if label == 1:\n",
        "        return 'cultural representative'\n",
        "    if label == 2:\n",
        "        return 'cultural exclusive'\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "dataset = WikiDataset()\n",
        "model = InferenceModel(\"fax4ever/culturalitems-roberta-base-5\", \"roberta-base\")\n",
        "\n",
        "tokenized_datasets = dataset.tokenize(model.tokenizer)\n",
        "print(tokenized_datasets)\n",
        "validation_ = tokenized_datasets[\"validation\"]\n",
        "\n",
        "matching = 0\n",
        "matching_ds = 0\n",
        "size = len(validation_)\n",
        "with open('transformer-inference.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    field = [\"item\", \"true label\", \"prediction\", \"prediction-ds\", \"correct\", \"correct-ds\"]\n",
        "    writer.writerow(field)\n",
        "    for index, item in enumerate(validation_):\n",
        "        p, p_ds = model.predict_text(item[\"description\"], item[\"wiki_text\"], item[\"input_ids\"], item[\"attention_mask\"])\n",
        "        true_label = item[\"label\"]\n",
        "        match = p == true_label\n",
        "        if match:\n",
        "            matching = matching + 1\n",
        "        match_ds = p_ds == true_label\n",
        "        if match_ds:\n",
        "            matching_ds = matching_ds + 1\n",
        "        writer.writerow([item[\"item\"], number_to_label(true_label), number_to_label(p), number_to_label(p_ds), match, match_ds])\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print('inference: ', index + 1, \"/\", size)\n",
        "            print('matched', matching, 'on', index + 1, '(', matching / (index + 1), ')')\n",
        "            print('matched', matching_ds, 'on', index + 1, '(', matching_ds / (index + 1), ')')\n",
        "print('inference: completed')\n",
        "print('matched', matching, 'on', size, '(', matching / size, ')')\n",
        "print('matched', matching_ds, 'on', size, '(', matching_ds / size, ')')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFNn9hz-CEjC",
        "outputId": "4f0239d8-fdc1-486b-b11a-f5a76f1941d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['item', 'name', 'description', 'type', 'category', 'subcategory', 'label', 'wiki_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 6251\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['item', 'name', 'description', 'type', 'category', 'subcategory', 'label', 'wiki_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "})\n",
            "inference:  10 / 300\n",
            "matched 8 on 10 ( 0.8 )\n",
            "matched 8 on 10 ( 0.8 )\n",
            "inference:  20 / 300\n",
            "matched 16 on 20 ( 0.8 )\n",
            "matched 16 on 20 ( 0.8 )\n",
            "inference:  30 / 300\n",
            "matched 23 on 30 ( 0.7666666666666667 )\n",
            "matched 23 on 30 ( 0.7666666666666667 )\n",
            "inference:  40 / 300\n",
            "matched 31 on 40 ( 0.775 )\n",
            "matched 31 on 40 ( 0.775 )\n",
            "inference:  50 / 300\n",
            "matched 40 on 50 ( 0.8 )\n",
            "matched 40 on 50 ( 0.8 )\n",
            "inference:  60 / 300\n",
            "matched 48 on 60 ( 0.8 )\n",
            "matched 48 on 60 ( 0.8 )\n",
            "inference:  70 / 300\n",
            "matched 57 on 70 ( 0.8142857142857143 )\n",
            "matched 57 on 70 ( 0.8142857142857143 )\n",
            "inference:  80 / 300\n",
            "matched 66 on 80 ( 0.825 )\n",
            "matched 66 on 80 ( 0.825 )\n",
            "inference:  90 / 300\n",
            "matched 74 on 90 ( 0.8222222222222222 )\n",
            "matched 74 on 90 ( 0.8222222222222222 )\n",
            "inference:  100 / 300\n",
            "matched 83 on 100 ( 0.83 )\n",
            "matched 83 on 100 ( 0.83 )\n",
            "inference:  110 / 300\n",
            "matched 90 on 110 ( 0.8181818181818182 )\n",
            "matched 90 on 110 ( 0.8181818181818182 )\n",
            "inference:  120 / 300\n",
            "matched 99 on 120 ( 0.825 )\n",
            "matched 99 on 120 ( 0.825 )\n",
            "inference:  130 / 300\n",
            "matched 107 on 130 ( 0.823076923076923 )\n",
            "matched 107 on 130 ( 0.823076923076923 )\n",
            "inference:  140 / 300\n",
            "matched 114 on 140 ( 0.8142857142857143 )\n",
            "matched 114 on 140 ( 0.8142857142857143 )\n",
            "inference:  150 / 300\n",
            "matched 122 on 150 ( 0.8133333333333334 )\n",
            "matched 122 on 150 ( 0.8133333333333334 )\n",
            "inference:  160 / 300\n",
            "matched 130 on 160 ( 0.8125 )\n",
            "matched 130 on 160 ( 0.8125 )\n",
            "inference:  170 / 300\n",
            "matched 136 on 170 ( 0.8 )\n",
            "matched 136 on 170 ( 0.8 )\n",
            "inference:  180 / 300\n",
            "matched 142 on 180 ( 0.7888888888888889 )\n",
            "matched 142 on 180 ( 0.7888888888888889 )\n",
            "inference:  190 / 300\n",
            "matched 151 on 190 ( 0.7947368421052632 )\n",
            "matched 151 on 190 ( 0.7947368421052632 )\n",
            "inference:  200 / 300\n",
            "matched 159 on 200 ( 0.795 )\n",
            "matched 159 on 200 ( 0.795 )\n",
            "inference:  210 / 300\n",
            "matched 164 on 210 ( 0.780952380952381 )\n",
            "matched 164 on 210 ( 0.780952380952381 )\n",
            "inference:  220 / 300\n",
            "matched 174 on 220 ( 0.7909090909090909 )\n",
            "matched 174 on 220 ( 0.7909090909090909 )\n",
            "inference:  230 / 300\n",
            "matched 184 on 230 ( 0.8 )\n",
            "matched 184 on 230 ( 0.8 )\n",
            "inference:  240 / 300\n",
            "matched 193 on 240 ( 0.8041666666666667 )\n",
            "matched 193 on 240 ( 0.8041666666666667 )\n",
            "inference:  250 / 300\n",
            "matched 198 on 250 ( 0.792 )\n",
            "matched 198 on 250 ( 0.792 )\n",
            "inference:  260 / 300\n",
            "matched 207 on 260 ( 0.7961538461538461 )\n",
            "matched 207 on 260 ( 0.7961538461538461 )\n",
            "inference:  270 / 300\n",
            "matched 216 on 270 ( 0.8 )\n",
            "matched 216 on 270 ( 0.8 )\n",
            "inference:  280 / 300\n",
            "matched 224 on 280 ( 0.8 )\n",
            "matched 224 on 280 ( 0.8 )\n",
            "inference:  290 / 300\n",
            "matched 233 on 290 ( 0.803448275862069 )\n",
            "matched 233 on 290 ( 0.803448275862069 )\n",
            "inference:  300 / 300\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "inference: completed\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n"
          ]
        }
      ]
    }
  ]
}