{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d338183bc964a829669a09385cf57a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c608d81ba5064324ad0ca5c64708f97f",
              "IPY_MODEL_0bb80d91844649f2b73d18d2eabf62c4",
              "IPY_MODEL_7fa4698139984f4fa89707de8668d1b9"
            ],
            "layout": "IPY_MODEL_256223a9126244e6bb0f2ec1a06f52b2"
          }
        },
        "c608d81ba5064324ad0ca5c64708f97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6abdd4f0c19447cb40e274e786df7f6",
            "placeholder": "​",
            "style": "IPY_MODEL_ab897510dadf459ebb6d85c52b21c336",
            "value": "Map: 100%"
          }
        },
        "0bb80d91844649f2b73d18d2eabf62c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7bb960474046928c85a55fe5052f6e",
            "max": 300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdb80d37ae3c4e82b5e1953053d71c6c",
            "value": 300
          }
        },
        "7fa4698139984f4fa89707de8668d1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b5e9abc7e7d4d769fa74d3cc8edd6c1",
            "placeholder": "​",
            "style": "IPY_MODEL_0dee9816b4344a3580c9b5ce29b647fb",
            "value": " 300/300 [00:03&lt;00:00, 87.45 examples/s]"
          }
        },
        "256223a9126244e6bb0f2ec1a06f52b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6abdd4f0c19447cb40e274e786df7f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab897510dadf459ebb6d85c52b21c336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c7bb960474046928c85a55fe5052f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdb80d37ae3c4e82b5e1953053d71c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b5e9abc7e7d4d769fa74d3cc8edd6c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dee9816b4344a3580c9b5ce29b647fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Transformer: Inference"
      ],
      "metadata": {
        "id": "JNPyhPqqQmv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikidata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DCNsUAXylqJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af62c02-1e59-438a-edfb-fe04b5b93b97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Wikidata in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random, requests, os, pickle\n",
        "import numpy as np\n",
        "from wikidata.client import Client\n",
        "from datasets import load_dataset\n",
        "from itertools import islice\n",
        "from google.colab import userdata\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "NPYS__k7tU3Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed"
      ],
      "metadata": {
        "id": "JPOI_M3umGch"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting seeds to try to make the training as much deterministic as possible.\n",
        "This should help to compare results (for the instance accuracy of the validation test) of different trainings."
      ],
      "metadata": {
        "id": "vQoa3pVfRZ_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "THzb1CV7ggEv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "c7HjzdTlmJZ-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia pages and Wikidata data need to be loaded from the web.\n",
        "In order to speed up the training and the inference, we store (cache) the retrieved and the processed data in files."
      ],
      "metadata": {
        "id": "UOP1h3JWRe4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "BASE_PATH = '/content/drive/MyDrive/Lost_in_Language_Recognition/'\n",
        "\n",
        "def dump(file_name, result):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    # remove dump files if present\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "    with open(file_path, 'wb') as file:\n",
        "        print(\"dumping\", file_path)\n",
        "        # noinspection PyTypeChecker\n",
        "        pickle.dump(result, file)\n",
        "\n",
        "def load(file_name):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    with open(file_path, 'rb') as file:\n",
        "        print(\"loading\", file_path)\n",
        "        # noinspection PyTypeChecker\n",
        "        return pickle.load(file)\n",
        "\n",
        "def file_exists(file_name):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    return os.path.exists(file_path)"
      ],
      "metadata": {
        "id": "xCas4n7JePkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c5d287-446a-428e-e2dc-0fa6ed720c43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we build the singleton `NLPDataset`, that contains:\n",
        "1. The original Hugging Face dataset\n",
        "2. The Wikidata entities\n",
        "3. The Wikipedia pages"
      ],
      "metadata": {
        "id": "YoUeopjcRkpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia_pages(sitelinks):\n",
        "    result = []\n",
        "    for site_key in sitelinks.keys():\n",
        "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "            lang = site_key.replace(\"wiki\", \"\")\n",
        "            result.append(lang)\n",
        "    return result\n",
        "\n",
        "def build_claims(claims):\n",
        "    result = {}\n",
        "    for prop_id, values in claims.items():\n",
        "        result[prop_id] = len(values)\n",
        "    return result\n",
        "\n",
        "class Entity:\n",
        "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
        "        self.entity_id = entity_id\n",
        "        self.label = dataset_item['label']\n",
        "        self.name = dataset_item['name']\n",
        "        self.description = dataset_item['description']\n",
        "        self.type = dataset_item['type']\n",
        "        self.category = dataset_item['category']\n",
        "        self.subcategory = dataset_item['subcategory']\n",
        "        self.wiki_text = wiki_text\n",
        "        # Languages\n",
        "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
        "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
        "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
        "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
        "        # Properties\n",
        "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
        "\n",
        "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wiki_text(en_wiki):\n",
        "    if not en_wiki:\n",
        "        return None\n",
        "    title = en_wiki[\"title\"]\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
        "    res = requests.get(API_URL, params=params, headers=headers)\n",
        "    json = res.json()\n",
        "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
        "    # Keep the original text as it is.\n",
        "    # The text will be processed in other methods,\n",
        "    # such as processed_dataset#tokenize().\n",
        "    return page.get(\"extract\", \"\")\n",
        "\n",
        "class EntityFactory:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, item):\n",
        "        entity_id = extract_entity_id(item['item'])\n",
        "        try:\n",
        "            wikidata = self.client.get(entity_id, load=True)\n",
        "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
        "            en_wiki = sitelinks.get(\"enwiki\")\n",
        "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
        "        except Exception as e:\n",
        "            print(\"Error loading id:\", entity_id, e)\n",
        "            return None\n",
        "\n",
        "TRAINING_FILE_NAME = \"training.bin\"\n",
        "VALIDATION_FILE_NAME = \"validation.bin\"\n",
        "\n",
        "def create_set(dataset, factory, limit, file_name):\n",
        "    # apply the limits\n",
        "    if limit is None:\n",
        "        limit = len(dataset)\n",
        "    result = []\n",
        "    for index, item in enumerate(islice(dataset, limit)):\n",
        "        created = factory.create(item)\n",
        "        if created is not None:\n",
        "            result.append(created)\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
        "    return result\n",
        "\n",
        "class NLPDataset:\n",
        "    def __init__(self, training_limit=None, validation_limit=None, force_reload=False):\n",
        "        if not (file_exists(TRAINING_FILE_NAME)) or not (file_exists(VALIDATION_FILE_NAME)) or force_reload:\n",
        "            # load the project dataset\n",
        "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HF_TOKEN'))\n",
        "            # a factory object is used to create our entities\n",
        "            factory = EntityFactory(Client())\n",
        "\n",
        "            self.training_set = create_set(dataset['train'], factory, training_limit, TRAINING_FILE_NAME)\n",
        "            self.validation_set = create_set(dataset['validation'], factory, validation_limit, VALIDATION_FILE_NAME)\n",
        "            dump(TRAINING_FILE_NAME, self.training_set)\n",
        "            dump(VALIDATION_FILE_NAME, self.validation_set)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.training_set = load(TRAINING_FILE_NAME)\n",
        "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set))"
      ],
      "metadata": {
        "id": "MnMKuBqNkBYn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_dataset = NLPDataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4hKm7wWbmwt1",
        "outputId": "65512b6b-d119-4317-a869-c87fad5095c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/training.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/validation.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dump files `training.bin` and `validation.bin` are present, the instance is build from the dump.\n",
        "And this is all we need to use the transformer. That is, usually with transformers we don't need to process the data."
      ],
      "metadata": {
        "id": "Cmm1ekIxRrlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "pUREAurO-f48"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we will keep the description and the Wikipedia page text as they are, leaving the burden of taking text processing decisions to the tokenizer. The dataset here is enriched with the Wikipedia text and the labels are mapped to numbers:"
      ],
      "metadata": {
        "id": "cCV8K37DR3mV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_entity_dict():\n",
        "    entity_dict = {}\n",
        "    for entity in nlp_dataset.training_set:\n",
        "        entity_dict[entity.entity_id] = entity\n",
        "    for entity in nlp_dataset.validation_set:\n",
        "        entity_dict[entity.entity_id] = entity\n",
        "    return entity_dict\n",
        "\n",
        "def label_to_number(label):\n",
        "    if label == 'cultural agnostic':\n",
        "        return 0\n",
        "    if label == 'cultural representative':\n",
        "        return 1\n",
        "    if label == 'cultural exclusive':\n",
        "        return 2\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "class WikiDataset:\n",
        "    def __init__(self):\n",
        "        entity_dict = build_entity_dict()\n",
        "        dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HF_TOKEN'))\n",
        "        # enriching the entities with the wiki pages\n",
        "        def map_labels(sample):\n",
        "            label = sample[\"label\"]\n",
        "            sample[\"label\"] = label_to_number(label)\n",
        "            wiki_id = extract_entity_id(sample[\"item\"])\n",
        "            if wiki_id is not None and wiki_id in entity_dict:\n",
        "                wiki_text = entity_dict[wiki_id].wiki_text\n",
        "                sample[\"wiki_text\"] = wiki_text if type(wiki_text) == str else \"\"\n",
        "            else:\n",
        "                sample[\"wiki_text\"] = \"\"\n",
        "            return sample\n",
        "        self.dataset = dataset.map(map_labels)\n",
        "\n",
        "    def tokenize(self, tokenizer):\n",
        "        def tokenize_function(items):\n",
        "            return tokenizer(items[\"description\"], items[\"wiki_text\"], padding=True, truncation=True)\n",
        "        return self.dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "6OETfZRXAlIo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab we reuse the result produced by the other colab: `NLP_yes_transformer_training.ipynb`. The model is used to predict the label of the validation set.\n",
        "\n",
        "Note: in the `predict_text` method there is a redundancy, we predict the labels using both the `input_ids` and the `attention_mask` tokenized on the fly and the ones that are created by the tokenization process of the dataset.\n",
        "We want to show the obvious: both should produce the same output."
      ],
      "metadata": {
        "id": "AEkIeczedhTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceModel:\n",
        "    def __init__(self, repo, kind):\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(repo)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(kind)\n",
        "        self.device = \"cpu\" if not torch.cuda.is_available() else \"cuda\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def predict_text(self, desc, wiki, input_ids_ds, attention_mask_ds):\n",
        "        self.model.eval()\n",
        "        # no max length - we want to use the default of the base model\n",
        "        # as we do in training\n",
        "        encoding = self.tokenizer(desc, wiki, return_tensors='pt', padding='max_length', truncation=True)\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "        input_ids_ds = torch.tensor(input_ids_ds).to(self.device).view(1, -1)\n",
        "        attention_mask_ds = torch.tensor(attention_mask_ds).to(self.device).view(1, -1)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids_ds, attention_mask=attention_mask_ds)\n",
        "            _, prediction = torch.max(outputs.logits, dim=1)\n",
        "            outputs2 = self.model(input_ids=input_ids_ds, attention_mask=attention_mask_ds)\n",
        "            _, prediction_ds = torch.max(outputs2.logits, dim=1)\n",
        "        return prediction.item(), prediction_ds.item()"
      ],
      "metadata": {
        "id": "JJ6X4DZuBmw8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the inference code. The output is also stored in a csv file: `transformer-inference.csv`. So that this can be inspected later."
      ],
      "metadata": {
        "id": "Q_WYVfZ-e1hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_label(label):\n",
        "    if label == 0:\n",
        "        return 'cultural agnostic'\n",
        "    if label == 1:\n",
        "        return 'cultural representative'\n",
        "    if label == 2:\n",
        "        return 'cultural exclusive'\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "dataset = WikiDataset()\n",
        "model = InferenceModel(\"fax4ever/culturalitems-roberta-base-5\", \"roberta-base\")\n",
        "\n",
        "tokenized_datasets = dataset.tokenize(model.tokenizer)\n",
        "print(tokenized_datasets)\n",
        "validation_ = tokenized_datasets[\"validation\"]\n",
        "\n",
        "matching = 0\n",
        "matching_ds = 0\n",
        "size = len(validation_)\n",
        "with open('transformer-inference.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    field = [\"item\", \"true label\", \"prediction\", \"prediction-ds\", \"correct\", \"correct-ds\"]\n",
        "    writer.writerow(field)\n",
        "    for index, item in enumerate(validation_):\n",
        "        p, p_ds = model.predict_text(item[\"description\"], item[\"wiki_text\"], item[\"input_ids\"], item[\"attention_mask\"])\n",
        "        true_label = item[\"label\"]\n",
        "        match = p == true_label\n",
        "        if match:\n",
        "            matching = matching + 1\n",
        "        match_ds = p_ds == true_label\n",
        "        if match_ds:\n",
        "            matching_ds = matching_ds + 1\n",
        "        writer.writerow([item[\"item\"], number_to_label(true_label), number_to_label(p), number_to_label(p_ds), match, match_ds])\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print('inference: ', index + 1, \"/\", size)\n",
        "            print('matched', matching, 'on', index + 1, '(', matching / (index + 1), ')')\n",
        "            print('matched', matching_ds, 'on', index + 1, '(', matching_ds / (index + 1), ')')\n",
        "print('inference: completed')\n",
        "print('matched', matching, 'on', size, '(', matching / size, ')')\n",
        "print('matched', matching_ds, 'on', size, '(', matching_ds / size, ')')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4d338183bc964a829669a09385cf57a0",
            "c608d81ba5064324ad0ca5c64708f97f",
            "0bb80d91844649f2b73d18d2eabf62c4",
            "7fa4698139984f4fa89707de8668d1b9",
            "256223a9126244e6bb0f2ec1a06f52b2",
            "a6abdd4f0c19447cb40e274e786df7f6",
            "ab897510dadf459ebb6d85c52b21c336",
            "0c7bb960474046928c85a55fe5052f6e",
            "bdb80d37ae3c4e82b5e1953053d71c6c",
            "1b5e9abc7e7d4d769fa74d3cc8edd6c1",
            "0dee9816b4344a3580c9b5ce29b647fb"
          ]
        },
        "id": "QFNn9hz-CEjC",
        "outputId": "8e0235bb-ebfc-48d4-ddb1-05dbcfca1a3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d338183bc964a829669a09385cf57a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['item', 'name', 'description', 'type', 'category', 'subcategory', 'label', 'wiki_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 6251\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['item', 'name', 'description', 'type', 'category', 'subcategory', 'label', 'wiki_text', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 300\n",
            "    })\n",
            "})\n",
            "inference:  10 / 300\n",
            "matched 8 on 10 ( 0.8 )\n",
            "matched 8 on 10 ( 0.8 )\n",
            "inference:  20 / 300\n",
            "matched 16 on 20 ( 0.8 )\n",
            "matched 16 on 20 ( 0.8 )\n",
            "inference:  30 / 300\n",
            "matched 23 on 30 ( 0.7666666666666667 )\n",
            "matched 23 on 30 ( 0.7666666666666667 )\n",
            "inference:  40 / 300\n",
            "matched 31 on 40 ( 0.775 )\n",
            "matched 31 on 40 ( 0.775 )\n",
            "inference:  50 / 300\n",
            "matched 40 on 50 ( 0.8 )\n",
            "matched 40 on 50 ( 0.8 )\n",
            "inference:  60 / 300\n",
            "matched 48 on 60 ( 0.8 )\n",
            "matched 48 on 60 ( 0.8 )\n",
            "inference:  70 / 300\n",
            "matched 57 on 70 ( 0.8142857142857143 )\n",
            "matched 57 on 70 ( 0.8142857142857143 )\n",
            "inference:  80 / 300\n",
            "matched 66 on 80 ( 0.825 )\n",
            "matched 66 on 80 ( 0.825 )\n",
            "inference:  90 / 300\n",
            "matched 74 on 90 ( 0.8222222222222222 )\n",
            "matched 74 on 90 ( 0.8222222222222222 )\n",
            "inference:  100 / 300\n",
            "matched 83 on 100 ( 0.83 )\n",
            "matched 83 on 100 ( 0.83 )\n",
            "inference:  110 / 300\n",
            "matched 90 on 110 ( 0.8181818181818182 )\n",
            "matched 90 on 110 ( 0.8181818181818182 )\n",
            "inference:  120 / 300\n",
            "matched 99 on 120 ( 0.825 )\n",
            "matched 99 on 120 ( 0.825 )\n",
            "inference:  130 / 300\n",
            "matched 107 on 130 ( 0.823076923076923 )\n",
            "matched 107 on 130 ( 0.823076923076923 )\n",
            "inference:  140 / 300\n",
            "matched 114 on 140 ( 0.8142857142857143 )\n",
            "matched 114 on 140 ( 0.8142857142857143 )\n",
            "inference:  150 / 300\n",
            "matched 122 on 150 ( 0.8133333333333334 )\n",
            "matched 122 on 150 ( 0.8133333333333334 )\n",
            "inference:  160 / 300\n",
            "matched 130 on 160 ( 0.8125 )\n",
            "matched 130 on 160 ( 0.8125 )\n",
            "inference:  170 / 300\n",
            "matched 136 on 170 ( 0.8 )\n",
            "matched 136 on 170 ( 0.8 )\n",
            "inference:  180 / 300\n",
            "matched 142 on 180 ( 0.7888888888888889 )\n",
            "matched 142 on 180 ( 0.7888888888888889 )\n",
            "inference:  190 / 300\n",
            "matched 151 on 190 ( 0.7947368421052632 )\n",
            "matched 151 on 190 ( 0.7947368421052632 )\n",
            "inference:  200 / 300\n",
            "matched 159 on 200 ( 0.795 )\n",
            "matched 159 on 200 ( 0.795 )\n",
            "inference:  210 / 300\n",
            "matched 164 on 210 ( 0.780952380952381 )\n",
            "matched 164 on 210 ( 0.780952380952381 )\n",
            "inference:  220 / 300\n",
            "matched 174 on 220 ( 0.7909090909090909 )\n",
            "matched 174 on 220 ( 0.7909090909090909 )\n",
            "inference:  230 / 300\n",
            "matched 184 on 230 ( 0.8 )\n",
            "matched 184 on 230 ( 0.8 )\n",
            "inference:  240 / 300\n",
            "matched 193 on 240 ( 0.8041666666666667 )\n",
            "matched 193 on 240 ( 0.8041666666666667 )\n",
            "inference:  250 / 300\n",
            "matched 198 on 250 ( 0.792 )\n",
            "matched 198 on 250 ( 0.792 )\n",
            "inference:  260 / 300\n",
            "matched 207 on 260 ( 0.7961538461538461 )\n",
            "matched 207 on 260 ( 0.7961538461538461 )\n",
            "inference:  270 / 300\n",
            "matched 216 on 270 ( 0.8 )\n",
            "matched 216 on 270 ( 0.8 )\n",
            "inference:  280 / 300\n",
            "matched 224 on 280 ( 0.8 )\n",
            "matched 224 on 280 ( 0.8 )\n",
            "inference:  290 / 300\n",
            "matched 233 on 290 ( 0.803448275862069 )\n",
            "matched 233 on 290 ( 0.803448275862069 )\n",
            "inference:  300 / 300\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "inference: completed\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n"
          ]
        }
      ]
    }
  ]
}