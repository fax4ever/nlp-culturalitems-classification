{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Transformer: Inference"
      ],
      "metadata": {
        "id": "JNPyhPqqQmv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikidata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DCNsUAXylqJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46087d83-70d5-4e66-9bc4-143a8fda7885"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Wikidata in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, random, requests, os, pickle\n",
        "import numpy as np\n",
        "from wikidata.client import Client\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "NPYS__k7tU3Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import set_seed"
      ],
      "metadata": {
        "id": "JPOI_M3umGch"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting seeds to try to make the training as much deterministic as possible.\n",
        "This should help to compare results (for the instance accuracy of the validation test) of different trainings."
      ],
      "metadata": {
        "id": "vQoa3pVfRZ_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "THzb1CV7ggEv"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42)"
      ],
      "metadata": {
        "id": "c7HjzdTlmJZ-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wikipedia pages and Wikidata data need to be loaded from the web.\n",
        "In order to speed up the training and the inference, we store (cache) the retrieved and the processed data in files."
      ],
      "metadata": {
        "id": "UOP1h3JWRe4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "BASE_PATH = '/content/drive/MyDrive/Lost_in_Language_Recognition/'\n",
        "\n",
        "def dump(file_name, result):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    # remove dump files if present\n",
        "    if os.path.exists(file_path):\n",
        "        os.remove(file_path)\n",
        "    with open(file_path, 'wb') as file:\n",
        "        print(\"dumping\", file_path)\n",
        "        # noinspection PyTypeChecker\n",
        "        pickle.dump(result, file)\n",
        "\n",
        "def load(file_name):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    with open(file_path, 'rb') as file:\n",
        "        print(\"loading\", file_path)\n",
        "        # noinspection PyTypeChecker\n",
        "        return pickle.load(file)\n",
        "\n",
        "def file_exists(file_name):\n",
        "    file_path = BASE_PATH + file_name\n",
        "    return os.path.exists(file_path)"
      ],
      "metadata": {
        "id": "xCas4n7JePkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c8865a9-7623-423d-d2c1-1c1a922ce848"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we build the singleton `NLPDataset`, that contains:\n",
        "1. The original Hugging Face dataset\n",
        "2. The Wikidata entities\n",
        "3. The Wikipedia pages"
      ],
      "metadata": {
        "id": "YoUeopjcRkpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wikipedia_pages(sitelinks):\n",
        "    result = []\n",
        "    for site_key in sitelinks.keys():\n",
        "        if site_key.endswith(\"wiki\") and not site_key.startswith(\"commons\"):\n",
        "            lang = site_key.replace(\"wiki\", \"\")\n",
        "            result.append(lang)\n",
        "    return result\n",
        "\n",
        "def build_claims(claims):\n",
        "    result = {}\n",
        "    for prop_id, values in claims.items():\n",
        "        result[prop_id] = len(values)\n",
        "    return result\n",
        "\n",
        "class Entity:\n",
        "    def __init__(self, entity_id, dataset_item, wiki_data, wiki_text):\n",
        "        self.entity_id = entity_id\n",
        "        if 'label' in dataset_item:\n",
        "            self.label = dataset_item['label']\n",
        "        self.name = dataset_item['name']\n",
        "        self.description = dataset_item['description']\n",
        "        self.type = dataset_item['type']\n",
        "        self.category = dataset_item['category']\n",
        "        self.subcategory = dataset_item['subcategory']\n",
        "        self.wiki_text = wiki_text\n",
        "        # Languages\n",
        "        self.labels = list(wiki_data.data.get(\"labels\", {}).keys())\n",
        "        self.descriptions = list(wiki_data.data.get(\"descriptions\", {}).keys())\n",
        "        self.aliases = list(wiki_data.data.get(\"aliases\", {}).keys())\n",
        "        self.wikipedia_pages = wikipedia_pages(wiki_data.data.get(\"sitelinks\", {}))\n",
        "        # Properties\n",
        "        self.claims = build_claims(wiki_data.data.get(\"claims\", {}))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.entity_id + \": \" + self.label + \" - \" + self.name\n",
        "\n",
        "API_URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "def extract_entity_id(url):\n",
        "    return url.strip().split(\"/\")[-1]\n",
        "\n",
        "def get_wiki_text(en_wiki):\n",
        "    if not en_wiki:\n",
        "        return None\n",
        "    title = en_wiki[\"title\"]\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"prop\": \"extracts\",\n",
        "        \"explaintext\": True,\n",
        "        \"titles\": title,\n",
        "        \"format\": \"json\",\n",
        "        \"redirects\": 1\n",
        "    }\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'}\n",
        "    res = requests.get(API_URL, params=params, headers=headers)\n",
        "    json = res.json()\n",
        "    page = next(iter(json[\"query\"][\"pages\"].values()))\n",
        "    # Keep the original text as it is.\n",
        "    # The text will be processed in other methods,\n",
        "    # such as processed_dataset#tokenize().\n",
        "    return page.get(\"extract\", \"\")\n",
        "\n",
        "class EntityFactory:\n",
        "    def __init__(self, client):\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, item):\n",
        "        entity_id = extract_entity_id(item['item'])\n",
        "        try:\n",
        "            wikidata = self.client.get(entity_id, load=True)\n",
        "            sitelinks = wikidata.data.get(\"sitelinks\", {})\n",
        "            en_wiki = sitelinks.get(\"enwiki\")\n",
        "            return Entity(entity_id, item, wikidata, get_wiki_text(en_wiki))\n",
        "        except Exception as e:\n",
        "            print(\"Error loading id:\", entity_id, e)\n",
        "            return None\n",
        "\n",
        "TEST_SET_FILE_NAME = BASE_PATH + \"test_unlabeled.csv\"\n",
        "\n",
        "TRAINING_FILE_NAME = \"training.bin\"\n",
        "VALIDATION_FILE_NAME = \"validation.bin\"\n",
        "TEST_FILE_NAME = \"test.bin\"\n",
        "\n",
        "def create_set(dataset, factory, file_name):\n",
        "    limit = len(dataset)\n",
        "    result = []\n",
        "    for index, item in enumerate(dataset):\n",
        "        created = factory.create(item)\n",
        "        if created is not None:\n",
        "            result.append(created)\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print(\"creating\", file_name, index + 1, \"/\", limit)\n",
        "    return result\n",
        "\n",
        "def load_or_create_set(factory, dataset, file_name):\n",
        "    if not (file_exists(file_name)):\n",
        "        created = create_set(dataset, factory, file_name)\n",
        "        dump(file_name, created)\n",
        "        return created\n",
        "    else:\n",
        "        return load(file_name)\n",
        "\n",
        "class NLPDataset:\n",
        "    def __init__(self):\n",
        "        if (not (file_exists(TRAINING_FILE_NAME)) or not (file_exists(VALIDATION_FILE_NAME))\n",
        "                or not (file_exists(TEST_FILE_NAME))):\n",
        "            # load the project dataset\n",
        "            dataset = load_dataset('sapienzanlp/nlp2025_hw1_cultural_dataset', token=userdata.get('HF_TOKEN'))\n",
        "            # a factory object is used to create our entities\n",
        "            factory = EntityFactory(Client())\n",
        "\n",
        "            self.training_set = load_or_create_set(factory, dataset['train'], TRAINING_FILE_NAME)\n",
        "            self.validation_set = load_or_create_set(factory, dataset['validation'], VALIDATION_FILE_NAME)\n",
        "            self.test_set = load_or_create_set(factory, pd.read_csv(TEST_SET_FILE_NAME).to_dict('records'), TEST_FILE_NAME)\n",
        "        else:\n",
        "            # by default load the dataset from a local dump\n",
        "            self.training_set = load(TRAINING_FILE_NAME)\n",
        "            self.validation_set = load(VALIDATION_FILE_NAME)\n",
        "            self.test_set = load(TEST_FILE_NAME)\n",
        "\n",
        "    def __str__(self):\n",
        "        return (\"training: \" + str(len(self.training_set)) + \". validation: \" + str(len(self.validation_set)) +\n",
        "                \". test: \" + str(len(self.test_set)))"
      ],
      "metadata": {
        "id": "MnMKuBqNkBYn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_dataset = NLPDataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4hKm7wWbmwt1",
        "outputId": "56c8813c-2047-42b7-a829-69e30aa94542"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/training.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/validation.bin\n",
            "loading /content/drive/MyDrive/Lost_in_Language_Recognition/test.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dump files `training.bin` and `validation.bin` are present, the instance is build from the dump.\n",
        "And this is all we need to use the transformer. That is, usually with transformers we don't need to process the data."
      ],
      "metadata": {
        "id": "Cmm1ekIxRrlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "pUREAurO-f48"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we will keep the description and the Wikipedia page text as they are, leaving the burden of taking text processing decisions to the tokenizer. The dataset here is enriched with the Wikipedia text and the labels are mapped to numbers:"
      ],
      "metadata": {
        "id": "cCV8K37DR3mV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab we reuse the result produced by the other colab: `NLP_yes_transformer_training.ipynb`. The model is used to predict the label of the validation set."
      ],
      "metadata": {
        "id": "AEkIeczedhTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceModel:\n",
        "    def __init__(self, repo, kind):\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(repo)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(kind)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def predict(self, desc, wiki):\n",
        "        self.model.eval()\n",
        "        # no max length - we want to use the default of the base model\n",
        "        # as we do in training\n",
        "        encoding = self.tokenizer(desc, wiki, return_tensors='pt', padding='max_length', truncation=True)\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, prediction = torch.max(outputs.logits, dim=1)\n",
        "        return prediction.item()"
      ],
      "metadata": {
        "id": "JJ6X4DZuBmw8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the inference code. The output is also stored in a csv file: `transformer-inference.csv`. So that this can be inspected later."
      ],
      "metadata": {
        "id": "Q_WYVfZ-e1hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def number_to_label(label):\n",
        "    if label == 0:\n",
        "        return 'cultural agnostic'\n",
        "    if label == 1:\n",
        "        return 'cultural representative'\n",
        "    if label == 2:\n",
        "        return 'cultural exclusive'\n",
        "    raise ValueError('label not suppoerted: ' + label)\n",
        "\n",
        "model = InferenceModel(\"fax4ever/culturalitems-roberta-base-5\", \"roberta-base\")\n",
        "\n",
        "matching = 0\n",
        "for index, item in enumerate(nlp_dataset.validation_set):\n",
        "    prediction = model.predict(item.description, item.wiki_text)\n",
        "    predicted_label = number_to_label(prediction)\n",
        "    match = predicted_label == item.label\n",
        "    if match:\n",
        "        matching = matching + 1\n",
        "    if (index + 1) % 10 == 0:\n",
        "        print('inference of the validation set: ', index + 1, \"/\", len(nlp_dataset.validation_set))\n",
        "        print('matched', matching, 'on', index + 1, '(', matching / (index + 1), ')')\n",
        "print('inference of the validation: completed')\n",
        "print('matched', matching, 'on', len(nlp_dataset.validation_set), '(', matching / len(nlp_dataset.validation_set), ')')\n",
        "\n",
        "with open(BASE_PATH + 'Lost_in_Language_Recognition_output_roberta.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    field = ['item', 'name', 'label']\n",
        "    writer.writerow(field)\n",
        "    for index, item in enumerate(nlp_dataset.test_set):\n",
        "        prediction = model.predict(item.description, item.wiki_text)\n",
        "        writer.writerow([\"http://www.wikidata.org/entity/\" + item.entity_id, item.name, number_to_label(prediction)])\n",
        "        if (index + 1) % 10 == 0:\n",
        "            print('inference of the test set: ', index + 1, \"/\", len(nlp_dataset.test_set))\n",
        "    print('inference of the test set: completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFNn9hz-CEjC",
        "outputId": "af051af0-d14f-47f9-a8da-0a13b61e6111"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inference of the validation set:  10 / 300\n",
            "matched 8 on 10 ( 0.8 )\n",
            "inference of the validation set:  20 / 300\n",
            "matched 16 on 20 ( 0.8 )\n",
            "inference of the validation set:  30 / 300\n",
            "matched 23 on 30 ( 0.7666666666666667 )\n",
            "inference of the validation set:  40 / 300\n",
            "matched 31 on 40 ( 0.775 )\n",
            "inference of the validation set:  50 / 300\n",
            "matched 40 on 50 ( 0.8 )\n",
            "inference of the validation set:  60 / 300\n",
            "matched 48 on 60 ( 0.8 )\n",
            "inference of the validation set:  70 / 300\n",
            "matched 57 on 70 ( 0.8142857142857143 )\n",
            "inference of the validation set:  80 / 300\n",
            "matched 66 on 80 ( 0.825 )\n",
            "inference of the validation set:  90 / 300\n",
            "matched 74 on 90 ( 0.8222222222222222 )\n",
            "inference of the validation set:  100 / 300\n",
            "matched 83 on 100 ( 0.83 )\n",
            "inference of the validation set:  110 / 300\n",
            "matched 90 on 110 ( 0.8181818181818182 )\n",
            "inference of the validation set:  120 / 300\n",
            "matched 99 on 120 ( 0.825 )\n",
            "inference of the validation set:  130 / 300\n",
            "matched 107 on 130 ( 0.823076923076923 )\n",
            "inference of the validation set:  140 / 300\n",
            "matched 114 on 140 ( 0.8142857142857143 )\n",
            "inference of the validation set:  150 / 300\n",
            "matched 122 on 150 ( 0.8133333333333334 )\n",
            "inference of the validation set:  160 / 300\n",
            "matched 130 on 160 ( 0.8125 )\n",
            "inference of the validation set:  170 / 300\n",
            "matched 136 on 170 ( 0.8 )\n",
            "inference of the validation set:  180 / 300\n",
            "matched 142 on 180 ( 0.7888888888888889 )\n",
            "inference of the validation set:  190 / 300\n",
            "matched 151 on 190 ( 0.7947368421052632 )\n",
            "inference of the validation set:  200 / 300\n",
            "matched 159 on 200 ( 0.795 )\n",
            "inference of the validation set:  210 / 300\n",
            "matched 164 on 210 ( 0.780952380952381 )\n",
            "inference of the validation set:  220 / 300\n",
            "matched 174 on 220 ( 0.7909090909090909 )\n",
            "inference of the validation set:  230 / 300\n",
            "matched 184 on 230 ( 0.8 )\n",
            "inference of the validation set:  240 / 300\n",
            "matched 193 on 240 ( 0.8041666666666667 )\n",
            "inference of the validation set:  250 / 300\n",
            "matched 198 on 250 ( 0.792 )\n",
            "inference of the validation set:  260 / 300\n",
            "matched 207 on 260 ( 0.7961538461538461 )\n",
            "inference of the validation set:  270 / 300\n",
            "matched 216 on 270 ( 0.8 )\n",
            "inference of the validation set:  280 / 300\n",
            "matched 224 on 280 ( 0.8 )\n",
            "inference of the validation set:  290 / 300\n",
            "matched 233 on 290 ( 0.803448275862069 )\n",
            "inference of the validation set:  300 / 300\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "inference of the validation: completed\n",
            "matched 239 on 300 ( 0.7966666666666666 )\n",
            "inference of the test set:  10 / 300\n",
            "inference of the test set:  20 / 300\n",
            "inference of the test set:  30 / 300\n",
            "inference of the test set:  40 / 300\n",
            "inference of the test set:  50 / 300\n",
            "inference of the test set:  60 / 300\n",
            "inference of the test set:  70 / 300\n",
            "inference of the test set:  80 / 300\n",
            "inference of the test set:  90 / 300\n",
            "inference of the test set:  100 / 300\n",
            "inference of the test set:  110 / 300\n",
            "inference of the test set:  120 / 300\n",
            "inference of the test set:  130 / 300\n",
            "inference of the test set:  140 / 300\n",
            "inference of the test set:  150 / 300\n",
            "inference of the test set:  160 / 300\n",
            "inference of the test set:  170 / 300\n",
            "inference of the test set:  180 / 300\n",
            "inference of the test set:  190 / 300\n",
            "inference of the test set:  200 / 300\n",
            "inference of the test set:  210 / 300\n",
            "inference of the test set:  220 / 300\n",
            "inference of the test set:  230 / 300\n",
            "inference of the test set:  240 / 300\n",
            "inference of the test set:  250 / 300\n",
            "inference of the test set:  260 / 300\n",
            "inference of the test set:  270 / 300\n",
            "inference of the test set:  280 / 300\n",
            "inference of the test set:  290 / 300\n",
            "inference of the test set:  300 / 300\n",
            "inference of the test set: completed\n"
          ]
        }
      ]
    }
  ]
}