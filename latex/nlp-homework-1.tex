\pdfoutput=1
\documentclass[11pt]{article}

\usepackage{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{numprint}
\usepackage{listings}

\title{Report MNLP}
%\date{May $4^{th}$, 2025}

\author{Ercoli Fabio Massimo \\
\texttt{802397} \\\And
Della Porta Nicolò \\
\texttt{1920468} \\\And
Regina Giovanni \\
\texttt{1972467} \\}

\begin{document}

	\maketitle

	\section{Introduction}
	Cultural items are elements such as concepts or entities that carry cultural meaning and reflect the identity, practices, and values of specific communities. In natural language, these items can appear in diverse forms, ranging from food names and historical references to gestures and works of art. Their interpretation often depends on shared knowledge within a culture, making their automatic classification a complex task.
	In this report, it is described how we addressed the task of automatic cultural item classification. The goal is to label each item by identifying the category it belongs to among the three given categories: \textit{Cultural Agnostic (CA)}, \textit{Cultural Representative (CR)} and \textit{Cultural Exclusive (CE)}. As requested, to tackle this we implemented and evaluated two distinct approaches: a LM-based method using an encoder Transformer and a non-LM-based method relying on several data. The report presents a comparative analysis of the two approaches in terms of classification performance and it explains how they work by reflecting on the methodological choices employed.

	\section{Methodology}

	\subsection{Non-LM-based}
	The main idea is very simple: using a FF (feed forward) shallow neural network to classify the items, using the labels to implement a typical supervised learning. For each entity we provide different kinds of information, for instance the Wikidata description, the Wikipedia page text, but also the set of languages for which we have a Wikipedia page, the category of the item (provided by the Homework dataset), but also the set of claims (attributes) defined in the Wikidata entry.
	The Wikidata description text is used twice as input, as frequency vector and is transformed into GloVe embeddings. We provide both as input.
	
	For each of those elements we build a frequency vector that have different dimensions, so we're using different dictionaries having different sizes. The idea we had is to re-scale each of original vector size to values that can be parameterize, in particular those re-scaled values are hyper parameters of the solution. So for each input we produce an embedding, and we concatenate all of those to create the input of the classifier, that is as we said, a FF (feed forward) shallow neural network. Each input embedding is a function of an input.
	
	Decide how to scale the input looks crucial to us, since the original frequency vector sizes are very different and we want make each input contributing with the right weight in order to classify well the entities. 
	
	\subsection{LM-based}
	The solution is built around a pretrained encoder that generates embeddings, which serve as the input to the classification network. We tokenize both the Wikidata descriptions and the English Wikipedia pages, experimenting with several English-only encoders. After multiple trials, we selected RoBERTa-base as the encoder, limiting the number of training epochs to optimize validation set accuracy.

	\section{Experiments}
	We use only the training set to train the network, while the validation set helps us evaluate how well the model generalizes to unseen data. Since the dataset is relatively balanced across labels, we focused on maximizing validation accuracy.
	
	During training, for each epoch, we track the loss and accuracy on both the training and validation sets. In particular, we monitor validation accuracy over epochs to determine the optimal point at which to stop training.
	
	At inference time, we generate label predictions for the entire validation set and compute the overall accuracy. We also predict labels for the test set and compile the results into a CSV file, as required by the assignment.
	
	\section{Results}
	
	For the non-LM-based classifier at inference time we get an overall accuracy of 0.7533333333333333, matching 226 on 300 items. For the LM-based classifier at inference time we get an overall accuracy of 0.7966666666666666, matching 239 on 300 items. 

	\appendix
	
	\section{Training results Appendix}
	\label{sec:appendix}
	
	\begin{figure}
		\includegraphics[width=\linewidth]{loss-no-transformer.png}
		\caption{non-LM-based classifier cross entropy loss.}
		\label{fig:1}
	\end{figure}
	
	\begin{figure}
		\includegraphics[width=\linewidth]{accuracy-no-transformer.png}
		\caption{non-LM-based classifier accuracy.}
		\label{fig:2}
	\end{figure}
	
	\begin{figure}
		\includegraphics[width=\linewidth]{loss-yes-transformer.png}
		\caption{LM-based classifier cross entropy loss.}
		\label{fig:3}
	\end{figure}
	
	\begin{figure}
		\includegraphics[width=\linewidth]{accuracy-yes-transformer.png}
		\caption{LM-based classifier accuracy.}
		\label{fig:4}
	\end{figure}
	
	\npdecimalsign{.}
	\nprounddigits{3}
	
	\begin{table}[]
		\small
		\caption{non-LM-based classifier training}
		\begin{tabular}{n{4}{2}|n{4}{2}|n{4}{2}|n{4}{2}}
			\toprule
			\multicolumn{1}{|l|}{train\_loss} & \multicolumn{1}{l|}{train\_accuracy} & \multicolumn{1}{l|}{valid\_loss} & \multicolumn{1}{l|}{valid\_accuracy} \\ \midrule
			0.824580834836376 & 0.6966198980808258 & 0.6848233610391616 & 0.6854166686534882 \\
			0.42828274567668534 & 0.8351084182457048 & 0.958229947090149 & 0.6864583313465118 \\
			0.22107240875080533 & 0.9225127551020408 & 1.5812558472156524 & 0.6979166686534881 \\
			0.13131998874406253 & 0.9583864795918368 & 2.4951072216033934 & 0.6864583313465118 \\
			0.130913822639945 & 0.9665178571428571 & 2.16357661485672 & 0.7114583313465118 \\
			0.09606347653952609 & 0.9752869897959183 & 2.5771642506122587 & 0.7291666686534881 \\
			0.07248458779450834 & 0.9835778061224489 & 2.981712055206299 & 0.69375 \\
			0.0265690988670643 & 0.9926658163265306 & 3.1195664286613463 & 0.7208333313465118 \\
			0.027661062501474775 & 0.9923469387755102 & 3.513991141319275 & 0.7083333313465119 \\
			0.029358040371947454 & 0.9947385204081632 & 3.4526482462882995 & 0.7020833313465118 \\
			0.09277717625173186 & 0.9904336734693877 & 3.629844832420349 & 0.6979166686534881 \\
			0.09905036353343255 & 0.9824617346938775 & 2.7048511862754823 & 0.690625 \\
			0.061935576511582345 & 0.9889987244897959 & 3.2353042364120483 & 0.7 \\
			0.05289996036179552 & 0.9885204081632653 & 4.332302975654602 & 0.678125 \\
			0.0905823381769124 & 0.9883609693877551 & 4.223013067245484 & 0.7229166686534881 \\
			0.04633352841285057 & 0.9925063775510204 & 3.9111754536628722 & 0.7229166686534881 \\
			0.03198182235851616 & 0.9929846938775511 & 5.171815228462219 & 0.7114583313465118 \\
			0.014295995866217713 & 0.9977678571428571 & 5.33933527469635 & 0.709375 \\
			0.02501944363913569 & 0.998405612244898 & 5.311948704719543 & 0.7416666686534882 \\
			\cmidrule(r){1-4}
		\end{tabular}
		\label{table:1}
	\end{table}
	
	\begin{table}[]
		\caption{LM-based classifier training}
		\begin{tabular}{n{4}{2}|n{4}{2}|n{4}{2}}
			\toprule
			\multicolumn{1}{|l|}{train\_loss} & \multicolumn{1}{l|}{valid\_loss} & \multicolumn{1}{l|}{valid\_accuracy} \\ \midrule
			null & 0.547339 & 0.796667 \\
			null & 0.569936 & 0.746667 \\
			0.571500 & 0.661266 & 0.753333 \\
			0.571500 & 0.730450 & 0.786667 \\
			\cmidrule(r){1-3}
		\end{tabular}
		\label{table:2}
	\end{table}
	
	\npnoround
	
	Figures \ref{fig:1} ,  \ref{fig:2} ,  \ref{fig:3}  and  \ref{fig:4} and Tables \ref{table:1} and \ref{table:2} present the cross entropy
	loss and accuracy for each epoch during the training phases of the two models.
	
	\section{Tested encoder models Appendix}
	\label{sec:appendix}
	
	\begin{table}[]
		\small
		\caption{LM-based classifier training}
		\begin{tabular}{@{}l|lll@{}}
			\toprule
			& \multicolumn{1}{l|}{batch\_size} & \multicolumn{1}{l|}{epochs} & \multicolumn{1}{l|}{length} \\ \midrule
			\multicolumn{1}{|l|}{google/bigbird-roberta-base} & 4                                & 4                           & 4096                             \\ \cmidrule(r){1-1}
			\multicolumn{1}{|l|}{distilbert-base-uncased}     & 32                               & 30                          & 512                              \\ \cmidrule(r){1-1}
			\multicolumn{1}{|l|}{roberta-base}                & 32                               & 30                          & 512                              \\ 
			\cmidrule(r){1-1}
			\multicolumn{1}{|l|}{roberta-large}                & 32                               & 30                          & 512                              \\ 
			\cmidrule(r){1-1}
			\multicolumn{1}{|l|}{xlm-roberta-base}            & 32                               & 30                          & 512                              \\ \cmidrule(r){1-1}
			\multicolumn{1}{|l|}{xlm-roberta-large}           & 32                               & 30                          & 512                              \\ \cmidrule(r){1-1}
			\multicolumn{1}{|l|}{microsoft/mdeberta-v3-base}  & 32                               & 30                          & 512                              \\ \cmidrule(r){1-1}
			\multicolumn{1}{|l|}{microsoft/mdeberta-v3-large} & 32                               & 30                          & 512                              \\ \bottomrule
		\end{tabular}
		\label{table:3}
	\end{table}
	
	Table \ref{table:3} lists the base encoder models we tested for the LM-based classifier. We selected RoBERTa-base based on a trade-off between performance and validation set accuracy. For instance, some models—such as BigBird—are too large to run efficiently on standard machines with typical GPUs.

	\section{File description Appendix}
	\label{sec:appendix}
	
	In this section we will describe the content of the \href{https://drive.google.com/drive/folders/1t5pgHkdenMFPdKqQweiFl8vhIAaiVFTM?usp=drive_link}{shared Google drive directory}.
	
	\subsection{Colab(s)}
	
	\emph{NLP\_no\_transformer\_training.ipynb}: non-LM-based training Colab, the trained model is pushed to the
	\href{https://huggingface.co/fax4ever/culturalitems-no-transformer}{Hugging Face repo for non-LM-based}.
	
	\noindent \emph{NLP\_no\_transformer\_inference.ipynb}: non-LM-based inference Colab, the trained model is pulled from the
	\href{https://huggingface.co/fax4ever/culturalitems-no-transformer}{Hugging Face repo for non-LM-based}.
	
	\noindent \emph{NLP\_yes\_transformer\_training.ipynb}: LM-based training Colab, the trained model is pushed to the
	\href{https://huggingface.co/fax4ever/culturalitems-roberta-base}{Hugging Face repo for LM-based}.
	
	\noindent \emph{NLP\_no\_transformer\_inference.ipynb}: LM-based inference Colab, the trained model is pulled from the
	\href{https://huggingface.co/fax4ever/culturalitems-roberta-base}{Hugging Face repo for LM-based}.
	
	\subsection{Data loaded dumps}

	These dumps contain data retrieved from Wikipedia pages and Wikidata entities online. 
	Since this content is mutable, results may vary if the files are regenerated. 
	If you want to reproduce the exact results described in the report, please use these files. 
	Remove them if your goal is to test the Colab’s ability to generate them from scratch:
	
	 \begin{itemize}
	 	\item training.bin
	 	\item validation.bin
	 	\item test.bin
	 \end{itemize}
	 
	 \subsection{Processed data dumps}
	 
	 These are used only by the non-LM-based components. Unlike the others, 
	 they can be safely deleted without affecting the results, 
	 as they simply store preprocessed data derived from the original sources. 
	 Use them to speed up training and inference. 
	 Remove them if your goal is to test Colab’s ability to recreate them:
	 
	 \begin{itemize}
	 	\item training-proc.bin
	 	\item validation-proc.bin
	 	\item test-proc.bin
	 \end{itemize}
	
	\subsection{Other files}
	
	\emph{Lost\_in\_Language\_Recognition\_output\_multimodalnn.csv}: non-LM-based inference result on the test set
	\emph{Lost\_in\_Language\_Recognition\_output\_roberta.csv}: LM-based inference result on the test set
	\emph{nlp-homework-1.pdf}: this report
	
	\section{Algorithms  Appendix}
	\label{sec:appendix}
	
	In this section we present some implementation ideas of the project, providing some
	code samples and describing them.
	
	\subsection{Pushing and Pulling Models from Hugging Face Repositories}
	
	For the LM-based component, we used Hugging Face’s Transformers high-level API. By using \emph{AutoModelForSequenceClassification}  the model instance provides a convenient 
	\emph{push\_to\_hub()} method for uploading the model to the Hugging Face Hub:
	
	\small
	\begin{lstlisting}[language=python]
model.push_to_hub(repo, token=...)
	\end{lstlisting}
	\normalsize
	
	To load the fine-tuned model, we simply use \emph{AutoModelForSequenceClassification.from\_pretrained}.
	
	Interestingly, we found that a similar workflow is possible even when using the lower-level PyTorch API, which we applied in the non-LM-based part. This requires the model class to extend the \emph{PyTorchModelHubMixin} interface.
	
	A few guidelines must be followed. For instance, all constructor parameters need to be serializable. Additionally, to ensure the model can be reused seamlessly on both CPU and CUDA devices, we recommend not passing the device type to the constructor. Instead, provide a method within the model to set the device for its components. You can refer to the  \emph{MultiModalModel} class used in the non-LM-based implementation as an example.
	
	The value of deploying the model to the Hub is that it becomes immutable and can be easily shared and tested across different environments.

\end{document}

